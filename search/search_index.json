{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Start \u00b6 Introduction \u00b6 This workshop curates the experience of setting up a cloud-native but vendor-agnostic GitSecOps pipeline that can be used to build, deliver, and manage microservices at any scale. Of course, this will not be a production-grade implementation but more of a reference architecture. It will allow the reader to explore the various facets of GitSecOps pipeline development and give them insight into how they may wish to pursue such an endeavor in their environments. At a high level, this is what we are looking to achieve: Skopeo Kaniko/Buildah Podman/Docker Nexus Helm Kind GitHub Tekton Tekton DockerHub ArgoCD Kubernetes | | | | | | | | | | | | ------------------------------------------------------------------------------------> DevOps - Creating Value and Availability +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | CODE | | BUILD | | TEST | | ARTIFACTS | | DEPLOY | | OPERATE | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ SecOps - Creating Trust & Confidence <------------------------------------------------------------------------------------- | | | | | | | | | | | | Semgrep Grype ZAP Snyk Trivy/Snyk Falco SonarQube ggshield SemGrep GLF Stack Gitleak Halolint SonarQube Metasploit Snyk Tekton Chains Trivy/Snyk Trivy DockerBench ZAP KubeBench Trivy/Snyk Nmap/Nessus Nikto/Zap Syft Additional Reading: Theory of GitSecOps GitSecOps is a practice that combines the version control capabilities of Git with security tools and processes to improve the security of an organization's software and infrastructure. By integrating security into the software development workflow, GitSecOps helps organizations ensure that their products are secure from the start, rather than trying to retroactively fix security issues after the fact. Some of the value of a GitSecOps pipeline are: Provides a clear record of all changes made to the codebase - Git keeps track of every change made to the codebase, along with the username of the person who made the change. This makes it easy to track down the source of any security issues that may arise, and to roll back changes if necessary. Leverages code reviews - Code review tools allow teams to review each other's code and catch any security issues before they are merged into the main codebase. Incorporates automated testing tools - Automated testing tools can run a suite of tests on the codebase to ensure that it meets security standards, and can alert developers if any issues are found. Makes it easier to manage and track access to sensitive code and infrastructure - By using Git and other tools to enforce access controls, organizations can ensure that only authorized users have access to sensitive information, reducing the risk of data breaches or other security incidents. Overall, GitSecOps helps organizations improve the security of their products by integrating security into the software development process, providing a clear record of changes, enforcing security policies through code review and automated testing, and managing access to sensitive information. By adopting GitSecOps practices, organization Implementing GitSecOps There is no one \"best\" way to implement GitSecOps, as the specific practices and tools that will work best for your organization will depend on your unique needs and goals. However, there are some general best practices that you can follow to help ensure the success of your GitSecOps efforts: Involve security experts in the process - Make sure that your GitSecOps team includes individuals with expertise in security. They can help identify and prioritize potential security risks and advise on the best tools and practices to address them. Automate as much as possible - Automating security checks and processes can help ensure that they are consistently and accurately applied, freeing up time for developers to focus on other tasks. Regularly review and update your GitSecOps practices - As your organization's needs and the security landscape evolve, it is important to regularly review and update your GitSecOps practices to ensure that they are still effective. Foster a culture of security - Encourage all team members to prioritize security in their work and make it a part of the company's overall culture. This can involve providing security training and making it easy for team members to report any potential security issues they encounter. Caveats and Exceptions This is a work in progress and is in a constant state of change and improvement. Please check back regularly. This is not meant to guide a production-ready environment. This workshop is experimental. As a result, many of the outcomes are clunky and theoretical. This is not meant to be a panacea. There are dozens of different tooling configurations that could be used. The tools used were picked based on several factors including but not limited to: 1) Acceptance by the CNCF, 2) Availability of books, tutorials, etc to expand learning, 3) Examples of use in production, 4)no/low cost) Not much time (if any) is spent teaching readers how to use the tools. Rather, we curate a selection of learning resources that participants should read through before completing any particular section. This workshop focuses on building a local environment in KIND only, but could be adapted to other settings by the participant. Principles of design \u00b6 When building out this reference pipeline, we use the following design principles: Security by design - Security is at the forefront of everything we do. It is not something we come back and bolt on later. Kubernetes first - Where possible, leverage tooling/mechanisms that use Kubernetes CRDs (reduces need to learn multiple configuration languages etc.) Open source - Whenever possible, we use and contribute to open source projects. Automate - After learning to do something by hand, we aim to automate. Everything as code - We aim to make it so everything we do is done as code. Cloud Native - We are building this to operate on ANY Cloud, whether public or private. Vendor Agnostic - The pipeline, with minor, tweaks should work everywhere. Progressions \u00b6 Crawl, Walk, Run, Fly \u00b6 Initially, there are 4 phases to work through: Crawl, Walk, Run, Fly. The \"crawl, walk, run\" methodology is an approach to introducing new concepts or capabilities gradually and incrementally. It involves starting with a basic, foundational level (crawl), then slowly building up to more advanced levels (walk), and ultimately achieving a level of proficiency or independence (run and fly ). It is essentially a way of introducing a new feature by starting with a basic prototype (crawl), then gradually adding more functionality and refining the design (walk), and ultimately releasing a fully-featured version of the feature (run and fly). Note Each stage above has 3-4 scenarios that are structured as follows: Introduction - High-level overview of the discovery. supplementary Reading - Links to tutorials and other information that provide knowledge on how to complete the scenario. Scenario - The problem or task at hand. Readers use knowledge from the supplementary reading to complete the task. Solution - A potential solution to the problem presented in the scenario. Challenge - An optional challenge that allows participants to test their understand of the concepts discovered. Crawl: In this phase we explore the basics of implementing a simple application first as a \"local\" (not running in containers) application and then later running in a container. Throughout these scenarios, we explore how to securely select and validate images before deploying. These are the scenarios explored during this stage: Local Application - How does one validate the code of an application and deploy locally? Containerized Application - What is the best way to choose and securley use containerized Applications and Images? Image From Scratch - How should one securely build a container image from scratch? Image Integrity - How does one validate the integrity of an image? Walk: We will explore using several deployment methods to deploy and manage containers in a Kubernetes environment. Basic security mechanisms for kubernetes will also be explored. Kind, Kaniko, and K8s - Set-up a local Kubernetes development clusters with Kind and deploy images using Kaniko. Build Images with Kaniko - Using Kaniko to build images without Docker Daemon Declarative Continuous Delivery - Using Declarative Continuous Delivery to manage deployment of an application.. Image Verification in K8s - Detecting and enforcing the use of signed images in K8s. Run: Reader will leverage Tekton to build a repeatable cloud native CI/CD system to deploy the application on Kubernetes. Tasks - Through a simple Hello World we learn the fundamentals of Tekton Pipelines - Expand on our use of Tekton and build an end-to-end build pipeline for the StudentBook application. Incorporating Security Testing - Add and automate the security testing we completed in the Crawl and Walk sections. Enforcing Integrity with Tekton - Using Tekton to enforce security through out the build and deploy phases. Fly: For this final step, we are going to go deeper into the end-to-end securing of the GitSecOps pipeline. Violent Pentesting - Use common tooling to complete a small pentest exercise Runtime Security - Implement basic runtime security for Kubernetes Security Chaos Engineering - Small example of chaos engineering Security testing in production - Run through security testing in a blue/green deployment scenario 2.2 Capstone \u00b6 The Capstone project is an opportunity to apply all the principles discovered during the workshop to a greenfield application. Readers are encouraged to use the application provided, but there is also the opportunity to use their own application. 2.3 Explorations \u00b6 Explorations allow readers to dive deeper into GitSecops, Microservices security, and much more. This is done by leveraging the book.info reference architecture and creating an exploration that others can follow. Once you are done with the previous stages, you will endeavor to write your own discovery and add it to the growing list of Explorations.","title":"Start"},{"location":"#start","text":"","title":"Start"},{"location":"#introduction","text":"This workshop curates the experience of setting up a cloud-native but vendor-agnostic GitSecOps pipeline that can be used to build, deliver, and manage microservices at any scale. Of course, this will not be a production-grade implementation but more of a reference architecture. It will allow the reader to explore the various facets of GitSecOps pipeline development and give them insight into how they may wish to pursue such an endeavor in their environments. At a high level, this is what we are looking to achieve: Skopeo Kaniko/Buildah Podman/Docker Nexus Helm Kind GitHub Tekton Tekton DockerHub ArgoCD Kubernetes | | | | | | | | | | | | ------------------------------------------------------------------------------------> DevOps - Creating Value and Availability +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ | CODE | | BUILD | | TEST | | ARTIFACTS | | DEPLOY | | OPERATE | +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ +-----------+ SecOps - Creating Trust & Confidence <------------------------------------------------------------------------------------- | | | | | | | | | | | | Semgrep Grype ZAP Snyk Trivy/Snyk Falco SonarQube ggshield SemGrep GLF Stack Gitleak Halolint SonarQube Metasploit Snyk Tekton Chains Trivy/Snyk Trivy DockerBench ZAP KubeBench Trivy/Snyk Nmap/Nessus Nikto/Zap Syft Additional Reading: Theory of GitSecOps GitSecOps is a practice that combines the version control capabilities of Git with security tools and processes to improve the security of an organization's software and infrastructure. By integrating security into the software development workflow, GitSecOps helps organizations ensure that their products are secure from the start, rather than trying to retroactively fix security issues after the fact. Some of the value of a GitSecOps pipeline are: Provides a clear record of all changes made to the codebase - Git keeps track of every change made to the codebase, along with the username of the person who made the change. This makes it easy to track down the source of any security issues that may arise, and to roll back changes if necessary. Leverages code reviews - Code review tools allow teams to review each other's code and catch any security issues before they are merged into the main codebase. Incorporates automated testing tools - Automated testing tools can run a suite of tests on the codebase to ensure that it meets security standards, and can alert developers if any issues are found. Makes it easier to manage and track access to sensitive code and infrastructure - By using Git and other tools to enforce access controls, organizations can ensure that only authorized users have access to sensitive information, reducing the risk of data breaches or other security incidents. Overall, GitSecOps helps organizations improve the security of their products by integrating security into the software development process, providing a clear record of changes, enforcing security policies through code review and automated testing, and managing access to sensitive information. By adopting GitSecOps practices, organization Implementing GitSecOps There is no one \"best\" way to implement GitSecOps, as the specific practices and tools that will work best for your organization will depend on your unique needs and goals. However, there are some general best practices that you can follow to help ensure the success of your GitSecOps efforts: Involve security experts in the process - Make sure that your GitSecOps team includes individuals with expertise in security. They can help identify and prioritize potential security risks and advise on the best tools and practices to address them. Automate as much as possible - Automating security checks and processes can help ensure that they are consistently and accurately applied, freeing up time for developers to focus on other tasks. Regularly review and update your GitSecOps practices - As your organization's needs and the security landscape evolve, it is important to regularly review and update your GitSecOps practices to ensure that they are still effective. Foster a culture of security - Encourage all team members to prioritize security in their work and make it a part of the company's overall culture. This can involve providing security training and making it easy for team members to report any potential security issues they encounter. Caveats and Exceptions This is a work in progress and is in a constant state of change and improvement. Please check back regularly. This is not meant to guide a production-ready environment. This workshop is experimental. As a result, many of the outcomes are clunky and theoretical. This is not meant to be a panacea. There are dozens of different tooling configurations that could be used. The tools used were picked based on several factors including but not limited to: 1) Acceptance by the CNCF, 2) Availability of books, tutorials, etc to expand learning, 3) Examples of use in production, 4)no/low cost) Not much time (if any) is spent teaching readers how to use the tools. Rather, we curate a selection of learning resources that participants should read through before completing any particular section. This workshop focuses on building a local environment in KIND only, but could be adapted to other settings by the participant.","title":"Introduction"},{"location":"#principles-of-design","text":"When building out this reference pipeline, we use the following design principles: Security by design - Security is at the forefront of everything we do. It is not something we come back and bolt on later. Kubernetes first - Where possible, leverage tooling/mechanisms that use Kubernetes CRDs (reduces need to learn multiple configuration languages etc.) Open source - Whenever possible, we use and contribute to open source projects. Automate - After learning to do something by hand, we aim to automate. Everything as code - We aim to make it so everything we do is done as code. Cloud Native - We are building this to operate on ANY Cloud, whether public or private. Vendor Agnostic - The pipeline, with minor, tweaks should work everywhere.","title":"Principles of design"},{"location":"#progressions","text":"","title":"Progressions"},{"location":"#crawl-walk-run-fly","text":"Initially, there are 4 phases to work through: Crawl, Walk, Run, Fly. The \"crawl, walk, run\" methodology is an approach to introducing new concepts or capabilities gradually and incrementally. It involves starting with a basic, foundational level (crawl), then slowly building up to more advanced levels (walk), and ultimately achieving a level of proficiency or independence (run and fly ). It is essentially a way of introducing a new feature by starting with a basic prototype (crawl), then gradually adding more functionality and refining the design (walk), and ultimately releasing a fully-featured version of the feature (run and fly). Note Each stage above has 3-4 scenarios that are structured as follows: Introduction - High-level overview of the discovery. supplementary Reading - Links to tutorials and other information that provide knowledge on how to complete the scenario. Scenario - The problem or task at hand. Readers use knowledge from the supplementary reading to complete the task. Solution - A potential solution to the problem presented in the scenario. Challenge - An optional challenge that allows participants to test their understand of the concepts discovered. Crawl: In this phase we explore the basics of implementing a simple application first as a \"local\" (not running in containers) application and then later running in a container. Throughout these scenarios, we explore how to securely select and validate images before deploying. These are the scenarios explored during this stage: Local Application - How does one validate the code of an application and deploy locally? Containerized Application - What is the best way to choose and securley use containerized Applications and Images? Image From Scratch - How should one securely build a container image from scratch? Image Integrity - How does one validate the integrity of an image? Walk: We will explore using several deployment methods to deploy and manage containers in a Kubernetes environment. Basic security mechanisms for kubernetes will also be explored. Kind, Kaniko, and K8s - Set-up a local Kubernetes development clusters with Kind and deploy images using Kaniko. Build Images with Kaniko - Using Kaniko to build images without Docker Daemon Declarative Continuous Delivery - Using Declarative Continuous Delivery to manage deployment of an application.. Image Verification in K8s - Detecting and enforcing the use of signed images in K8s. Run: Reader will leverage Tekton to build a repeatable cloud native CI/CD system to deploy the application on Kubernetes. Tasks - Through a simple Hello World we learn the fundamentals of Tekton Pipelines - Expand on our use of Tekton and build an end-to-end build pipeline for the StudentBook application. Incorporating Security Testing - Add and automate the security testing we completed in the Crawl and Walk sections. Enforcing Integrity with Tekton - Using Tekton to enforce security through out the build and deploy phases. Fly: For this final step, we are going to go deeper into the end-to-end securing of the GitSecOps pipeline. Violent Pentesting - Use common tooling to complete a small pentest exercise Runtime Security - Implement basic runtime security for Kubernetes Security Chaos Engineering - Small example of chaos engineering Security testing in production - Run through security testing in a blue/green deployment scenario","title":"Crawl, Walk, Run, Fly"},{"location":"#22-capstone","text":"The Capstone project is an opportunity to apply all the principles discovered during the workshop to a greenfield application. Readers are encouraged to use the application provided, but there is also the opportunity to use their own application.","title":"2.2 Capstone"},{"location":"#23-explorations","text":"Explorations allow readers to dive deeper into GitSecops, Microservices security, and much more. This is done by leveraging the book.info reference architecture and creating an exploration that others can follow. Once you are done with the previous stages, you will endeavor to write your own discovery and add it to the growing list of Explorations.","title":"2.3 Explorations"},{"location":"capstone/","text":"Capstone \u00b6 Introduction \u00b6 The purpose of a capstone project is to demonstrate your mastery of implementing the GitSecOps pipeline in a more practical setting. This may require you to conduct additional research and will definitely require you to synthesize and integrate the skills and knowledge they have gained throughout this workshop. Additionally, completing this capstone project will prepare you to give back to the GitSecOps by preparing you to participate in the next level of the workshop( Explorations) Bookinfo Application \u00b6 The BookInfo application is at the heart of the Capstone project. It is a simple web application that displays information about a book, including its title, author, and publication date. It is built using a microservices architecture and includes several independent components that work together to deliver the functionality. BookInfo application additional details The application was created as a way to demonstrate the use of microservices and how they can be used to build scalable and resilient applications. In this introduction, we will provide an overview of the components of the BookInfo application and explain how they fit together to form the overall application. We will also discuss some of the key concepts and technologies used in the application, including microservices, containers, and Kubernetes. The BookInfo application is built using a microservices architecture, which means that it is composed of a set of independent, modular services that communicate with each other to deliver the desired functionality. Here is a list of the components that are included in the BookInfo application: Product Catalog Service: This service stores a list of books and their associated information, such as title, author, and publication date. Rating Service: This service stores ratings and reviews for books. Details Service: This service retrieves information about a specific book from the Product Catalog Service and retrieves ratings and reviews for the book from the Rating Service. It then combines this information and returns it to the user. Review Service: This service retrieves ratings and reviews for books from the Rating Service and displays them to the user. Web UI: This is the user interface for the BookInfo application. It allows users to browse and search for books, view detailed information about a particular book, and rate and review books. In addition to the core BookInfo application, there are also several additional components that are used to demonstrate various concepts and capabilities. For example, there is a version of the application that includes an additional service that demonstrates how to use an external database to store data. There is also a version that includes a load balancer and demonstrates how to route traffic to different versions of a service. Requirement \u00b6 You must design and deploy (locally or in cloud) an end-to-end Kubernetes application based on the BookInfo application using the GitSecOps pipeline built throughout this workshop At a high-level, this project should implement the following: A forked/clone repo of the BookInfo application A threat model (optional) Documented in a git repository in a repeatable fashion Deploy a Tekton pipeline that automates the build and deployment of the application in a declarative deployment approach (e.g. ArgoCD) Implements security testing and validation at all stages of the pipeline Add one unique aspect to your architecture (e.g service mesh, artifact server, external secrets management). You can use an existing exploration or create a new one. A reflection Completing the reflection \u00b6 A reflection is for yourself and does not have to be provided to anyone, but it can be useful to complete one to really to solidify your learning. You may choose the format for your reflection. Possible formats include: * Written essay * Video * Interview (record on video) * Digital Presentation (prezi, Google slides, etc.) * Mindmap Some sample questions you can ask yourself include but are not limited to: What process did you go through to produce the exploration? Did everything work the way you had planned? What lessons did you learn? Describe your successes and frustrations. What new skills did you learn while doing this work? Did you have any interesting or unique experiences while completing your exploration? What part of your exploration are you most proud of, and why? What was the hardest part of doing this exploration? What were your challenges? What mistakes did you make, and what did you do to limit the impact or shift your course? What kinds of support could you have used during the process? If you could do anything about the exploration over again, what, if anything, would you change? What advice would you give to other readers who are preparing to embark on the capstone process?","title":"Capstone"},{"location":"capstone/#capstone","text":"","title":"Capstone"},{"location":"capstone/#introduction","text":"The purpose of a capstone project is to demonstrate your mastery of implementing the GitSecOps pipeline in a more practical setting. This may require you to conduct additional research and will definitely require you to synthesize and integrate the skills and knowledge they have gained throughout this workshop. Additionally, completing this capstone project will prepare you to give back to the GitSecOps by preparing you to participate in the next level of the workshop( Explorations)","title":"Introduction"},{"location":"capstone/#bookinfo-application","text":"The BookInfo application is at the heart of the Capstone project. It is a simple web application that displays information about a book, including its title, author, and publication date. It is built using a microservices architecture and includes several independent components that work together to deliver the functionality. BookInfo application additional details The application was created as a way to demonstrate the use of microservices and how they can be used to build scalable and resilient applications. In this introduction, we will provide an overview of the components of the BookInfo application and explain how they fit together to form the overall application. We will also discuss some of the key concepts and technologies used in the application, including microservices, containers, and Kubernetes. The BookInfo application is built using a microservices architecture, which means that it is composed of a set of independent, modular services that communicate with each other to deliver the desired functionality. Here is a list of the components that are included in the BookInfo application: Product Catalog Service: This service stores a list of books and their associated information, such as title, author, and publication date. Rating Service: This service stores ratings and reviews for books. Details Service: This service retrieves information about a specific book from the Product Catalog Service and retrieves ratings and reviews for the book from the Rating Service. It then combines this information and returns it to the user. Review Service: This service retrieves ratings and reviews for books from the Rating Service and displays them to the user. Web UI: This is the user interface for the BookInfo application. It allows users to browse and search for books, view detailed information about a particular book, and rate and review books. In addition to the core BookInfo application, there are also several additional components that are used to demonstrate various concepts and capabilities. For example, there is a version of the application that includes an additional service that demonstrates how to use an external database to store data. There is also a version that includes a load balancer and demonstrates how to route traffic to different versions of a service.","title":"Bookinfo Application"},{"location":"capstone/#requirement","text":"You must design and deploy (locally or in cloud) an end-to-end Kubernetes application based on the BookInfo application using the GitSecOps pipeline built throughout this workshop At a high-level, this project should implement the following: A forked/clone repo of the BookInfo application A threat model (optional) Documented in a git repository in a repeatable fashion Deploy a Tekton pipeline that automates the build and deployment of the application in a declarative deployment approach (e.g. ArgoCD) Implements security testing and validation at all stages of the pipeline Add one unique aspect to your architecture (e.g service mesh, artifact server, external secrets management). You can use an existing exploration or create a new one. A reflection","title":"Requirement"},{"location":"capstone/#completing-the-reflection","text":"A reflection is for yourself and does not have to be provided to anyone, but it can be useful to complete one to really to solidify your learning. You may choose the format for your reflection. Possible formats include: * Written essay * Video * Interview (record on video) * Digital Presentation (prezi, Google slides, etc.) * Mindmap Some sample questions you can ask yourself include but are not limited to: What process did you go through to produce the exploration? Did everything work the way you had planned? What lessons did you learn? Describe your successes and frustrations. What new skills did you learn while doing this work? Did you have any interesting or unique experiences while completing your exploration? What part of your exploration are you most proud of, and why? What was the hardest part of doing this exploration? What were your challenges? What mistakes did you make, and what did you do to limit the impact or shift your course? What kinds of support could you have used during the process? If you could do anything about the exploration over again, what, if anything, would you change? What advice would you give to other readers who are preparing to embark on the capstone process?","title":"Completing the reflection"},{"location":"crawl/","text":"Container Images \u00b6 Introduction \u00b6 For the crawl stage the focus is on first installing applications locally and then securely converting these to image containers using a distroless base container and running them through a battery of security testing and signing the images to prove providence. Distroless containers are a type of container image that do not include a Linux distribution, such as Alpine or Ubuntu. Instead, they only contain the necessary libraries and dependencies to run a specific application. This can lead to smaller image sizes and improved security by reducing the attack surface. +----------------------+ +----------------------+ +----------------------+ | | |Verify the distroless | | Build app container | | GitHub +------>|container base images +----->|images with distroless| | | | using Cosign before | | verfied base images | | | | build | | | +----------------------+ +----------------------+ +----------+-----------+ | | | v +----------------------+ +----------------------+ +----------------------+ | | | | | | | Verify the container | | Cosign the app | | Container image | | image using Cosign |<-----+ container image |<-----+ scanning | | | | | | | +----------------------+ +----------------------+ +----------------------+ The ways to ensure the integrity of images include but are not limited to: Digital signatures - Container images can be digitally signed to ensure that they have not been tampered with. By verifying the signature before running the image, you can ensure that the image has not been modified since it was signed. Image scanning - Tools such as Clair or Aqua Security's MicroScanner can be used to scan container images for known vulnerabilities and other issues. This can help identify any tampering or issues with the image that might compromise its integrity. Image signing and scanning during the build process - By signing and scanning images as part of the build process, you can ensure that the images are secure and have not been tampered with before they are published. Regular updates - It is important to regularly update container images to ensure that they include the latest security patches and updates. This can help protect against vulnerabilities that might have been introduced since the image was originally published. We will be using to simple applications to help us explore these scenarios: JuiceShop - OWASP Juice Shop is probably the most modern and sophisticated insecure web application! It can be used in security trainings, awareness demos, CTFs and as a guinea pig for security tools StudentBook - a small mciroservice based on FastAPI. simpler than Juiceshop and will give readers a chance to explore some of the intricacies of containerization. Scenarios \u00b6 Local Application - Download and deploy JuiceShop locally (no containers) and run through the basics of application security. Containerized Application - Practice validating and verifying a third party image. The downloading and publishing the application in a container, securely choosing and using a container published by someone else Image from Scratch - Securely building a container from scratch using Podman, Skopeo, Buildah, and Kaniko Image Integrity - Validating the integrity of our scratch containers","title":"Container Images"},{"location":"crawl/#container-images","text":"","title":"Container Images"},{"location":"crawl/#introduction","text":"For the crawl stage the focus is on first installing applications locally and then securely converting these to image containers using a distroless base container and running them through a battery of security testing and signing the images to prove providence. Distroless containers are a type of container image that do not include a Linux distribution, such as Alpine or Ubuntu. Instead, they only contain the necessary libraries and dependencies to run a specific application. This can lead to smaller image sizes and improved security by reducing the attack surface. +----------------------+ +----------------------+ +----------------------+ | | |Verify the distroless | | Build app container | | GitHub +------>|container base images +----->|images with distroless| | | | using Cosign before | | verfied base images | | | | build | | | +----------------------+ +----------------------+ +----------+-----------+ | | | v +----------------------+ +----------------------+ +----------------------+ | | | | | | | Verify the container | | Cosign the app | | Container image | | image using Cosign |<-----+ container image |<-----+ scanning | | | | | | | +----------------------+ +----------------------+ +----------------------+ The ways to ensure the integrity of images include but are not limited to: Digital signatures - Container images can be digitally signed to ensure that they have not been tampered with. By verifying the signature before running the image, you can ensure that the image has not been modified since it was signed. Image scanning - Tools such as Clair or Aqua Security's MicroScanner can be used to scan container images for known vulnerabilities and other issues. This can help identify any tampering or issues with the image that might compromise its integrity. Image signing and scanning during the build process - By signing and scanning images as part of the build process, you can ensure that the images are secure and have not been tampered with before they are published. Regular updates - It is important to regularly update container images to ensure that they include the latest security patches and updates. This can help protect against vulnerabilities that might have been introduced since the image was originally published. We will be using to simple applications to help us explore these scenarios: JuiceShop - OWASP Juice Shop is probably the most modern and sophisticated insecure web application! It can be used in security trainings, awareness demos, CTFs and as a guinea pig for security tools StudentBook - a small mciroservice based on FastAPI. simpler than Juiceshop and will give readers a chance to explore some of the intricacies of containerization.","title":"Introduction"},{"location":"crawl/#scenarios","text":"Local Application - Download and deploy JuiceShop locally (no containers) and run through the basics of application security. Containerized Application - Practice validating and verifying a third party image. The downloading and publishing the application in a container, securely choosing and using a container published by someone else Image from Scratch - Securely building a container from scratch using Podman, Skopeo, Buildah, and Kaniko Image Integrity - Validating the integrity of our scratch containers","title":"Scenarios"},{"location":"crawl/local/","text":"1.Local Application \u00b6 Introduction \u00b6 For a start, we will install JuiceShop locally and interact with it as a \"traditional\" application without the context of containers or microservices. This will allow us to become familiar with the application from the ground up. It will also let us lay the foundation for the tooling required for the appropriate security testing level. Here is a brief overview of the application we will use We suggest that you Walk the happy path before getting started - https://pwning.owasp-juice.shop/part1/happy-path.html A big focus of this section is understanding the importance of security scanning your container images. Security scanning container images involves checking the image for known vulnerabilities and compliance issues. This can be done at several levels: With standalone container scanning using ooling like Aqua Security, Snyk, and Anchore. With traditional vulnerability scanning tools like Nessus, OpenVAS, and Qualys. Using built-in scanning capabilities in Image registries such as DockerHub, Quay, and Harbor. These tools and services check the image for known vulnerabilities in the operating system and application dependencies, as well as for compliance with security standards and best practices. They also check for any sensitive information or misconfigurations in the image that could compromise the container environment's security. It is important to note that security scanning is not a one-time process. It should be done regularly, especially when new version of the images are pushed to the registry. For now, you can dump all the analysis into a folder called outputs (e.g. /tmp/juice/outputs). In the future, we will redirect outputs to a system that can aggregate, enrich, and normalize the findings. We will not do anything with these findings now, but reviewing them is a good idea so you can understand the outputs. Supplementary Learning Material \u00b6 Additional Links: \u00b6 The OWASP Juiceshop project - https://owasp.org/www-project-juice-shop/ Hacking Juiceshop the DevSecOps way - https://www.omerlh.info/2018/12/23/hacking-juice-shop-the-devsecops-way Pwning Juiceshop - https://pwning.owasp-juice.shop/ Learning SemGrep - https://semgrep.dev/learn Semgrep and Juiceshop - https://www.linkedin.com/pulse/brief-semgrep-analyse-juice-shop-bar%C4%B1%C5%9F-ekin-y%C4%B1ld%C4%B1r%C4%B1m Semgrep Introduction - https://payatu.com/blog/vishnu.k/semgrep-introduction Snyk and Juiceshop - https://gitlab.com/snyk-rfrazier/juice-shop NMAP - https://www.edureka.co/blog/nmap-tutorial/ Nikto - https://techyrick.com/nikto-2/ Scenario \u00b6 Install Juice Shop Locally Complete a secure static code analysis with SemGrep Run a software composite analysis with Snyk Run Git secret analysis with Gitleaks Run Infrastructure scan (port) with NMAP Run Web scan with Nikto Solution Solution 1.Install JuiceShop locally. # Make sure node is installed $ git clone https://github.com/juice-shop/juice-shop.git --depth 1 $ cd juice-shop $ npm install $ npm start # Browse to http://localhost:3000 # Take sometime to walk the happy path - https://pwning.owasp-juice.shop/part1/happy-path.html 2.Complete a secure static code analysis with SemGrep # Set-up a file structure to handle tests. These can be anywhere/anything. $ mkdir /tmp/juice && mkdir /tmp/juice/outputs # Git Clone Juiceshop to testing folders $ cd /tmp/juice $ git clone git@github.com:juice-shop/juice-shop.git $ cd juice-shop # Run basic Semgrep analysis $ docker run --rm -v \"${PWD}:/src\" returntocorp/semgrep semgrep --config=auto | tee /tmp/juice/outputs/semgrep.txt 3.Run a software composite analysis with Snyk Snyk is not opensource. However, they do have a free tier that you can use once you sign-up. # Make sure you are in the Juice-shop dir that you cloned earlier $ CD /tmp/juice/juice-shop $ snyk test | tee /tmp/juice/outputs/snyk.txt 4.Run Git secret analysis with Gitleaks $ docker pull zricethezav/gitleaks:latest $ docker run -v ${path_to_host_folder_to_scan}:/path zricethezav/gitleaks:latest [COMMAND] --source=\"/path\" [OPTIONS] 5.Run Infrastructure scan (port) with NMAP # Of course, there are better tools (e.g. ZAP) but this will give you the rough idea of how to run an infrastructure scan $ docker pull instrumentisto/nmap $ docker run --rm -it instrumentisto/nmap -A -T4 scanme.nmap.org | tee /tmp/juice/outputs/nmap.txt 6.Run Web scan with Nikto # Of course, there are better tools (e.g. ZAP) but this will give you the rough idea of how to run a web scan $ docker pull frapsoft/nikto $ docker run frapsoft/nikto -host https://example.com | tee /tmp/juice/outputs/nikto.txt Additional Challenges \u00b6 PWN OWASP JuiceShop! - Complete all the tasks and challenges to completely hack the JuiceShop. Here is your inspiration https://pwning.owasp-juice.shop/ Semi-automated security check - Write a Bash file that semi-automates the security check process used in this scenario.","title":"1.Local Application"},{"location":"crawl/local/#1local-application","text":"","title":"1.Local Application"},{"location":"crawl/local/#introduction","text":"For a start, we will install JuiceShop locally and interact with it as a \"traditional\" application without the context of containers or microservices. This will allow us to become familiar with the application from the ground up. It will also let us lay the foundation for the tooling required for the appropriate security testing level. Here is a brief overview of the application we will use We suggest that you Walk the happy path before getting started - https://pwning.owasp-juice.shop/part1/happy-path.html A big focus of this section is understanding the importance of security scanning your container images. Security scanning container images involves checking the image for known vulnerabilities and compliance issues. This can be done at several levels: With standalone container scanning using ooling like Aqua Security, Snyk, and Anchore. With traditional vulnerability scanning tools like Nessus, OpenVAS, and Qualys. Using built-in scanning capabilities in Image registries such as DockerHub, Quay, and Harbor. These tools and services check the image for known vulnerabilities in the operating system and application dependencies, as well as for compliance with security standards and best practices. They also check for any sensitive information or misconfigurations in the image that could compromise the container environment's security. It is important to note that security scanning is not a one-time process. It should be done regularly, especially when new version of the images are pushed to the registry. For now, you can dump all the analysis into a folder called outputs (e.g. /tmp/juice/outputs). In the future, we will redirect outputs to a system that can aggregate, enrich, and normalize the findings. We will not do anything with these findings now, but reviewing them is a good idea so you can understand the outputs.","title":"Introduction"},{"location":"crawl/local/#supplementary-learning-material","text":"","title":"Supplementary Learning Material"},{"location":"crawl/local/#additional-links","text":"The OWASP Juiceshop project - https://owasp.org/www-project-juice-shop/ Hacking Juiceshop the DevSecOps way - https://www.omerlh.info/2018/12/23/hacking-juice-shop-the-devsecops-way Pwning Juiceshop - https://pwning.owasp-juice.shop/ Learning SemGrep - https://semgrep.dev/learn Semgrep and Juiceshop - https://www.linkedin.com/pulse/brief-semgrep-analyse-juice-shop-bar%C4%B1%C5%9F-ekin-y%C4%B1ld%C4%B1r%C4%B1m Semgrep Introduction - https://payatu.com/blog/vishnu.k/semgrep-introduction Snyk and Juiceshop - https://gitlab.com/snyk-rfrazier/juice-shop NMAP - https://www.edureka.co/blog/nmap-tutorial/ Nikto - https://techyrick.com/nikto-2/","title":"Additional Links:"},{"location":"crawl/local/#scenario","text":"Install Juice Shop Locally Complete a secure static code analysis with SemGrep Run a software composite analysis with Snyk Run Git secret analysis with Gitleaks Run Infrastructure scan (port) with NMAP Run Web scan with Nikto Solution Solution 1.Install JuiceShop locally. # Make sure node is installed $ git clone https://github.com/juice-shop/juice-shop.git --depth 1 $ cd juice-shop $ npm install $ npm start # Browse to http://localhost:3000 # Take sometime to walk the happy path - https://pwning.owasp-juice.shop/part1/happy-path.html 2.Complete a secure static code analysis with SemGrep # Set-up a file structure to handle tests. These can be anywhere/anything. $ mkdir /tmp/juice && mkdir /tmp/juice/outputs # Git Clone Juiceshop to testing folders $ cd /tmp/juice $ git clone git@github.com:juice-shop/juice-shop.git $ cd juice-shop # Run basic Semgrep analysis $ docker run --rm -v \"${PWD}:/src\" returntocorp/semgrep semgrep --config=auto | tee /tmp/juice/outputs/semgrep.txt 3.Run a software composite analysis with Snyk Snyk is not opensource. However, they do have a free tier that you can use once you sign-up. # Make sure you are in the Juice-shop dir that you cloned earlier $ CD /tmp/juice/juice-shop $ snyk test | tee /tmp/juice/outputs/snyk.txt 4.Run Git secret analysis with Gitleaks $ docker pull zricethezav/gitleaks:latest $ docker run -v ${path_to_host_folder_to_scan}:/path zricethezav/gitleaks:latest [COMMAND] --source=\"/path\" [OPTIONS] 5.Run Infrastructure scan (port) with NMAP # Of course, there are better tools (e.g. ZAP) but this will give you the rough idea of how to run an infrastructure scan $ docker pull instrumentisto/nmap $ docker run --rm -it instrumentisto/nmap -A -T4 scanme.nmap.org | tee /tmp/juice/outputs/nmap.txt 6.Run Web scan with Nikto # Of course, there are better tools (e.g. ZAP) but this will give you the rough idea of how to run a web scan $ docker pull frapsoft/nikto $ docker run frapsoft/nikto -host https://example.com | tee /tmp/juice/outputs/nikto.txt","title":"Scenario"},{"location":"crawl/local/#additional-challenges","text":"PWN OWASP JuiceShop! - Complete all the tasks and challenges to completely hack the JuiceShop. Here is your inspiration https://pwning.owasp-juice.shop/ Semi-automated security check - Write a Bash file that semi-automates the security check process used in this scenario.","title":"Additional Challenges"},{"location":"crawl/published/","text":"2.Containerized Application \u00b6 Introduction \u00b6 Before one uses a 3rd party image/container, one should ensure that they thoroughly inspect and validate the integrity of the image . Scanning a container image before using it is crucial because it allows you to identify any known vulnerabilities in the image that attackers could potentially exploit. These vulnerabilities can exist in the base operating system or in any packages installed in the image. Scanning the image also allows you to verify that the image has not been tampered with and is what the publisher intended. An even better approach is to re-build 3rd party container images using the Dockerfile, rather than just downloading and using an image from an external container registry. There are several reasons why you might choose to build a container image from a Dockerfile rather than use an existing image from Docker Hub: Customization Building an image from a Dockerfile allows you to customize the image to include exactly the packages and configurations that you need. This can be useful if you need to add specific packages or make other customizations that are not included in the existing images on Docker Hub. Version Control By building an image from a Dockerfile, you can store the Dockerfile in version control along with your application code. This allows you to track changes to the image over time and roll back to previous versions if necessary. Reproducibility Building an image from a Dockerfile allows you to ensure that the image is built consistently every time. This can be useful if you need to deploy the image in multiple environments and want to ensure that the image is the same in each environment. Security Building an image from a Dockerfile allows you to audit the exact steps that were used to build the image. This can be helpful for security purposes, as you can review the commands in the Dockerfile to ensure that they do not include any vulnerabilities. Overall, building an image from a Dockerfile can provide you with more control and flexibility when deploying your applications with Docker. Supplementary Learning Material \u00b6 Installations: Installing Trivy for container introspection - https://aquasecurity.github.io/trivy/v0.18.3/installation/ Installing Skopeo for image interaction - https://github.com/containers/skopeo/blob/main/install.md Installing Podman for containers - https://podman.io/getting-started/installation Installing Buildah for building containers from scratch - https://github.com/containers/buildah/blob/main/install.md) Container Security Testing: Cli tools for containers - https://snyk.io/blog/command-line-tools-for-containers/ Testing with Trivy - https://semaphoreci.com/blog/continuous-container-vulnerability-testing-with-trivy Containers and Semgrep - https://kondukto.io/blog/docker-security-best-practices-with-semgrep Dockerfile best practices - https://github.com/kondukto-io/dockerfile-bestpractice-rules Using hadolint - https://github.com/hadolint/hadolint Hadolint - https://www.containiq.com/post/hadolint Syft and Grype - https://medium.com/rahasak/container-vulnerability-scan-with-syft-and-grype-f4ec9cd4d7f1 Trivy https://medium.com/ascentic-technology/secure-container-images-with-trivy-1ef12b5b9b4d Scenario \u00b6 Inspect Image on GitHub Repo with Trivy Inspect the Dockerfile with Hadolint Check for Secrets with gggshield Inspect with Skopeo on DockerHub Download Image as OCI Archive and scan with Grype Run Software Composite Analysis with Syft Pull Juiceshop image and rerun security testing Check for misconfigurations with DockerBench Run container and scan with NMAP and Nikto Solution Solution 1.Inspect Image on GitHub Repo with Trivy - Look for security problems BEFORE you download one bit of the image. $ trivy repo https://github.com/juice-shop/juice-shop | tee /tmp/juice/outputs/trivy.txt #Note try this to see what it would look like if results were available $ trivy repo https://github.com/knqyf263/trivy-ci-test 2.Inspect the Dockerfile with Hadolint - Easy way to catch some misconfigurations before you download an image from DockerHub and/or rebuild from a Dockerfile # Clone repo $ git clone https://github.com/juice-shop/juice-shop.git # Download and install Hadolint $ docker pull ghcr.io/hadolint/hadolint # Run Hadolint $ docker run --rm -i ghcr.io/hadolint/hadolint < Dockerfile | tee /tmp/juice/outputs/hadolint.txt 3.Check for Secrets with gggshield - Note: This particular tool is not opensource but free for low use scenarios. You will need to sign-up before using. $ export GITGUARDIAN_API_KEY=the-token-you-got-from-dashboard $ python3 -m venv /tmp/juice/venv $ /tmp//juice/venv/bin/pip install ggshield $ ggshield scan docker bad-secrets 4.Inspect with Skopeo on DockerHub - Again before downloading, look for integrity issues before downloading. $ skopeo inspect docker://bkimminich/juice-shop | jq '.' | tee /tmp/juice/outputs/skopeo.txt # To just see the config $ skopeo inspect --config docker://bkimminich/juice-shop | jq '.' # list just the tags skopeo list-tags docker://bkimminich/juice-shop 5.Download Image as OCI Archive and scan with Grype - Downloading as an archived TAR to a isolated location. Not pulling the image just yet. We want to check for known vulnerabilities. # Copy image to local OCI Archive $ skopeo copy docker://bkimminich/juice-shop oci-archive:/tmp/juice/juice-shop.tar # Install Grype $ curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sudo sh -s -- -b /usr/local/bin # Scan with Grype $ grype oci-archive:/tmp/juice/juice-shop.tar --scope all-layers | tee /tmp/juice/outputs/grype.txt # Bonus Snyk has great tooling to do this. You will need to set-up a free tier account. $ snyk container test oci-archive:/tmp/juice/juice-shop.tar 6.Run Software Composite Analysis with Syft - Output a SBOM for use in upstream process and to see what dependencies are in play. # Install Syft $ curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sudo sh -s -- -b /usr/local/bin # Generate SBOM from Image $ syft packages docker.io/bkimminich/juice-shop --scope all-layers -o json # Generate SBOM from Archieve $ syft packages oci-archive:/tmp/juice/juice-shop.tar --scope all-layers -o json | tee /tmp/juice/outputs/syft.txt 7.Pull Juiceshop image and rerun security testing - Image has been thoroughly tested even before downloading. Now we can download it and run testing again. This is overkill but shows that most of the testing can also be run against the local image, not just an archive. $ docker pull bkimminich/juice-shop # Scan with Grype $ grype docker.io/bkimminich/juice-shop --scope all-layers # Scan with Trivy $ trivy image bkimminich/juice-shop # Run Syft $ syft packages docker.io/bkimminich/juice-shop --scope all-layers -o json 8.Check for misconfigurations with DockerBench - Do this before running the image. $ cd /tmp/juice $ git clone https://github.com/docker/docker-bench-security.git $ cd docker-bench-security $ ./docker-bench-security.sh -i juice-shop | tee /tmp/juice/outputs/bench.txt 9.Run container and scan with NMAP and Nikto - There are much better commercial and free scanners to use but this is a good starting point to see how DAST can be incorporated early on in the pipeline. # Run the docker container $ docker run --rm -p 3000:3000 bkimminich/juice-shop # Get tooling docker pull instrumentisto/nmap docker pull securecodebox/nmap docker pull frapsoft/nikto # Get Container IP Address (when running locally) # Docker ps #get container ID docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id # Scan with NMAp $ docker run --rm -it instrumentisto/nmap -A -T4 172.17.0.3 -p 3000 | tee /tmp/juice/outputs/nmap.txt $ docker run --rm -it frapsoft/nikto -h 172.17.0.3:3000 Additional Challenges \u00b6 Automate - Semi-automate your 3rd party container image security assessment process with a Bash script Better Tooling - Add some bigger and broader tooling to the security verification pipeline. ADD OWASP Zap as an additional web scanning testing tool. https://github.com/zaproxy/zaproxy Implement OPENVas alongside Nessus. https://github.com/greenbone/openvas-scanner Include Snyk Scanning for software composite analysis as part of the workflow. https://snyk.io/","title":"2.Containerized Application"},{"location":"crawl/published/#2containerized-application","text":"","title":"2.Containerized Application"},{"location":"crawl/published/#introduction","text":"Before one uses a 3rd party image/container, one should ensure that they thoroughly inspect and validate the integrity of the image . Scanning a container image before using it is crucial because it allows you to identify any known vulnerabilities in the image that attackers could potentially exploit. These vulnerabilities can exist in the base operating system or in any packages installed in the image. Scanning the image also allows you to verify that the image has not been tampered with and is what the publisher intended. An even better approach is to re-build 3rd party container images using the Dockerfile, rather than just downloading and using an image from an external container registry. There are several reasons why you might choose to build a container image from a Dockerfile rather than use an existing image from Docker Hub: Customization Building an image from a Dockerfile allows you to customize the image to include exactly the packages and configurations that you need. This can be useful if you need to add specific packages or make other customizations that are not included in the existing images on Docker Hub. Version Control By building an image from a Dockerfile, you can store the Dockerfile in version control along with your application code. This allows you to track changes to the image over time and roll back to previous versions if necessary. Reproducibility Building an image from a Dockerfile allows you to ensure that the image is built consistently every time. This can be useful if you need to deploy the image in multiple environments and want to ensure that the image is the same in each environment. Security Building an image from a Dockerfile allows you to audit the exact steps that were used to build the image. This can be helpful for security purposes, as you can review the commands in the Dockerfile to ensure that they do not include any vulnerabilities. Overall, building an image from a Dockerfile can provide you with more control and flexibility when deploying your applications with Docker.","title":"Introduction"},{"location":"crawl/published/#supplementary-learning-material","text":"Installations: Installing Trivy for container introspection - https://aquasecurity.github.io/trivy/v0.18.3/installation/ Installing Skopeo for image interaction - https://github.com/containers/skopeo/blob/main/install.md Installing Podman for containers - https://podman.io/getting-started/installation Installing Buildah for building containers from scratch - https://github.com/containers/buildah/blob/main/install.md) Container Security Testing: Cli tools for containers - https://snyk.io/blog/command-line-tools-for-containers/ Testing with Trivy - https://semaphoreci.com/blog/continuous-container-vulnerability-testing-with-trivy Containers and Semgrep - https://kondukto.io/blog/docker-security-best-practices-with-semgrep Dockerfile best practices - https://github.com/kondukto-io/dockerfile-bestpractice-rules Using hadolint - https://github.com/hadolint/hadolint Hadolint - https://www.containiq.com/post/hadolint Syft and Grype - https://medium.com/rahasak/container-vulnerability-scan-with-syft-and-grype-f4ec9cd4d7f1 Trivy https://medium.com/ascentic-technology/secure-container-images-with-trivy-1ef12b5b9b4d","title":"Supplementary Learning Material"},{"location":"crawl/published/#scenario","text":"Inspect Image on GitHub Repo with Trivy Inspect the Dockerfile with Hadolint Check for Secrets with gggshield Inspect with Skopeo on DockerHub Download Image as OCI Archive and scan with Grype Run Software Composite Analysis with Syft Pull Juiceshop image and rerun security testing Check for misconfigurations with DockerBench Run container and scan with NMAP and Nikto Solution Solution 1.Inspect Image on GitHub Repo with Trivy - Look for security problems BEFORE you download one bit of the image. $ trivy repo https://github.com/juice-shop/juice-shop | tee /tmp/juice/outputs/trivy.txt #Note try this to see what it would look like if results were available $ trivy repo https://github.com/knqyf263/trivy-ci-test 2.Inspect the Dockerfile with Hadolint - Easy way to catch some misconfigurations before you download an image from DockerHub and/or rebuild from a Dockerfile # Clone repo $ git clone https://github.com/juice-shop/juice-shop.git # Download and install Hadolint $ docker pull ghcr.io/hadolint/hadolint # Run Hadolint $ docker run --rm -i ghcr.io/hadolint/hadolint < Dockerfile | tee /tmp/juice/outputs/hadolint.txt 3.Check for Secrets with gggshield - Note: This particular tool is not opensource but free for low use scenarios. You will need to sign-up before using. $ export GITGUARDIAN_API_KEY=the-token-you-got-from-dashboard $ python3 -m venv /tmp/juice/venv $ /tmp//juice/venv/bin/pip install ggshield $ ggshield scan docker bad-secrets 4.Inspect with Skopeo on DockerHub - Again before downloading, look for integrity issues before downloading. $ skopeo inspect docker://bkimminich/juice-shop | jq '.' | tee /tmp/juice/outputs/skopeo.txt # To just see the config $ skopeo inspect --config docker://bkimminich/juice-shop | jq '.' # list just the tags skopeo list-tags docker://bkimminich/juice-shop 5.Download Image as OCI Archive and scan with Grype - Downloading as an archived TAR to a isolated location. Not pulling the image just yet. We want to check for known vulnerabilities. # Copy image to local OCI Archive $ skopeo copy docker://bkimminich/juice-shop oci-archive:/tmp/juice/juice-shop.tar # Install Grype $ curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sudo sh -s -- -b /usr/local/bin # Scan with Grype $ grype oci-archive:/tmp/juice/juice-shop.tar --scope all-layers | tee /tmp/juice/outputs/grype.txt # Bonus Snyk has great tooling to do this. You will need to set-up a free tier account. $ snyk container test oci-archive:/tmp/juice/juice-shop.tar 6.Run Software Composite Analysis with Syft - Output a SBOM for use in upstream process and to see what dependencies are in play. # Install Syft $ curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sudo sh -s -- -b /usr/local/bin # Generate SBOM from Image $ syft packages docker.io/bkimminich/juice-shop --scope all-layers -o json # Generate SBOM from Archieve $ syft packages oci-archive:/tmp/juice/juice-shop.tar --scope all-layers -o json | tee /tmp/juice/outputs/syft.txt 7.Pull Juiceshop image and rerun security testing - Image has been thoroughly tested even before downloading. Now we can download it and run testing again. This is overkill but shows that most of the testing can also be run against the local image, not just an archive. $ docker pull bkimminich/juice-shop # Scan with Grype $ grype docker.io/bkimminich/juice-shop --scope all-layers # Scan with Trivy $ trivy image bkimminich/juice-shop # Run Syft $ syft packages docker.io/bkimminich/juice-shop --scope all-layers -o json 8.Check for misconfigurations with DockerBench - Do this before running the image. $ cd /tmp/juice $ git clone https://github.com/docker/docker-bench-security.git $ cd docker-bench-security $ ./docker-bench-security.sh -i juice-shop | tee /tmp/juice/outputs/bench.txt 9.Run container and scan with NMAP and Nikto - There are much better commercial and free scanners to use but this is a good starting point to see how DAST can be incorporated early on in the pipeline. # Run the docker container $ docker run --rm -p 3000:3000 bkimminich/juice-shop # Get tooling docker pull instrumentisto/nmap docker pull securecodebox/nmap docker pull frapsoft/nikto # Get Container IP Address (when running locally) # Docker ps #get container ID docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' container_name_or_id # Scan with NMAp $ docker run --rm -it instrumentisto/nmap -A -T4 172.17.0.3 -p 3000 | tee /tmp/juice/outputs/nmap.txt $ docker run --rm -it frapsoft/nikto -h 172.17.0.3:3000","title":"Scenario"},{"location":"crawl/published/#additional-challenges","text":"Automate - Semi-automate your 3rd party container image security assessment process with a Bash script Better Tooling - Add some bigger and broader tooling to the security verification pipeline. ADD OWASP Zap as an additional web scanning testing tool. https://github.com/zaproxy/zaproxy Implement OPENVas alongside Nessus. https://github.com/greenbone/openvas-scanner Include Snyk Scanning for software composite analysis as part of the workflow. https://snyk.io/","title":"Additional Challenges"},{"location":"crawl/scratch/","text":"3.Image From Scratch \u00b6 Introduction \u00b6 This scenario assumes that you have completed the security verification practices in the last exercise set. If you have not done this, you should ensure you complete these steps and understand the importance of inspecting an image before using it. For this scenario, we will recreate an image container from scratch and push it to a container repository and then pull it for use locally. There are several reasons why someone might want to build a Docker image from scratch: Customization Building a container image from scratch allows you to fully customize the contents of the image to meet your specific needs. You can include only the libraries, dependencies, and applications that you need, and configure the image in exactly the way that you want. Size Building a container image from scratch can result in a smaller final image size, as you only include the components that you need and can optimize the image for your specific use case. Security Building a Container image from scratch allows you to start with a minimal base image and add only the components that you need, which can help to reduce the attack surface and improve security Control Building a Docker image from scratch gives you complete control over the contents and configuration of the image, which can be useful if you have specific requirements or constraints. Learning Building a Docker image from scratch can also be a useful learning exercise, as it allows you to understand the process of creating a Docker image and the various components that go into it. Single vs multi stage image building \u00b6 We will also explore building single-stage and multistage container images. A single-stage container build is a process in which all the necessary steps to create a container image are performed in a single build stage. This can include building the application, installing dependencies, and packaging the final image. On the other hand, a multistage container build involves creating the container image in multiple stages. Each stage in the build process includes only the packages and libraries necessary for that stage rather than the entire build environment. The final stage of the build process creates the final container image, which only includes the packages and libraries needed for the application. There are several reasons why it can be beneficial to build containers in multiple stages: Smaller image size Building a container in multiple stages allows you to create smaller final images. This is because each stage in the build process only includes the packages and libraries necessary for that stage, rather than the entire build environment. This can make it easier to deploy and manage the container, as well as reduce the risk of security vulnerabilities. Improved Security Building a container in multiple stages can improve security by reducing the attack surface of the final image. This is because the final image only includes the packages and libraries necessary for the application, rather than the entire build environment. Simplified maintenance Building a container in multiple stages can make it easier to maintain the container over time. This is because you can update or rebuild individual stages of the build process without affecting the entire container. Improved build efficiency Building a container in multiple stages can improve build efficiency by allowing you to reuse common build steps across multiple containers. This can save time and reduce the complexity of the build process. Overall, building containers in multiple stages can provide a number of benefits, including smaller image size, improved security, simplified maintenance, and improved build efficiency. Supplementary Learning Material \u00b6 Additional Links: \u00b6 Podman for Devops Chapter 6 - https://learning.oreilly.com/library/view/podman-for-devops/9781803248233/B17908_06_epub.xhtml#_idParaDest-131 Buildah as a cli tool - https://hackernoon.com/ Getting started Buildah - <(https://opensource.com/article/18/6/getting-started-buildah> Buildah and OCI Images - https://www.linode.com/docs/guides/using-buildah-oci-images/ Github buildah tutorial - https://github.com/containers/buildah/blob/main/docs/tutorials/01-intro.md) Docker Multistage - https://docs.docker.com/build/building/multi-stage Buildah vs Kaniko - https://earthly.dev/blog/docker-vs-buildah-vs-kaniko/ Distroless - https://github.com/GoogleContainerTools/distroless distroless - https://medium.com/@luke_perry_dev/dockerizing-with-distroless-f3b84ae10f3a Scenario \u00b6 Install and Test Student API Application Create a Basic Dockerfile Create a Multistage DockerFile Create a Single Stage Buildah File Creat a MultiStage Buildah File Push image to Dockerhub Solutions Solution 1.Install and Test Student API Application 1.1 Set-up environment $ mkdir student $ cd student $ sudo apt-get install python3-venv $ python3 -m venv venv $ source venv/bin/activate $ pip install fastapi uvicorn $ sudo apt-get install jq 1.2 Create Application cat <<'EOF' >>app.py # from fastapi import FastAPI, HTTPException from typing import Optional from pydantic import BaseModel app = FastAPI() students = [ {'name': 'Student 1', 'age': 20}, {'name': 'Student 2', 'age': 18}, {'name': 'Student 3', 'age': 16} ] class Student(BaseModel): name: str age: int @app.get('/students') def user_list(min: Optional[int] = None, max: Optional[int] = None): if min and max: filtered_students = list( filter(lambda student: max >= student['age'] >= min, students) ) return {'students': filtered_students} return {'students': students} @app.get('/students/{student_id}') def user_detail(student_id: int): student_check(student_id) return {'student': students[student_id]} @app.post('/students') def user_add(student: Student): students.append(student) return {'student': students[-1]} @app.put('/students/{student_id}') def user_update(student: Student, student_id: int): student_check(student_id) students[student_id].update(student) return {'student': students[student_id]} @app.delete('/students/{student_id}') def user_delete(student_id: int): student_check(student_id) del students[student_id] return {'students': students} def student_check(student_id): if not students[student_id]: raise HTTPException(status_code=404, detail='Student Not Found') EOF 1.3 Run application uvicorn --port 8090 app:app # Create the requirements.txt for next stages $ pip freeze > requirements.txt 1.4 Test the API # Get all students $ curl http://127.0.0.1:8090/students # Filter students based on age $ curl \"http://127.0.0.1:8090/students?min=16&max=18\" # Get single student $ curl http://127.0.0.1:8090/students/0T # Add student $ curl -X 'POST' http://127.0.0.1:8090/students -H 'Content-Type: application/json' -d '{\"name\":\"Tekton Operator\", \"age\": 99}' # update student $ curl -X 'PUT' http://127.0.0.1:8090/students/0 -H 'Content-Type: application/json' -d '{\"name\":\"Student X\", \"age\": 18}' # Delete Student $curl -X 'DELETE' http://127.0.0.1:8090/students/3 2.0 Create a Basic Dockerfile cat <<'EOF' >>Dockerfile FROM python:3.8 # Install dependencies COPY requirements.txt . RUN pip install -r requirements.txt # Copy the application code COPY . . # Run the application CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8090\"] EOF 2.1 Build and run application $ docker build -t studentbook . $ docker run -p 8090:8090 --name sb studentbook 3.0 Create a Multistage DockerFile cat <<'EOF' >>Dockerfile # Stage 1 - Build stage FROM python:3.10-slim AS build-env # Copy the application code COPY . /app WORKDIR /app # Install dependencies RUN pip install --no-cache-dir --upgrade -r requirements.txt && cp $(which uvicorn) /app # Stage 2 - Distroless container FROM gcr.io/distroless/python3 # Copy the built application from the build stage COPY --from=build-env /app /app COPY --from=build-env /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages ENV PYTHONPATH=/usr/local/lib/python3.10/site-packages WORKDIR /app # Run the application CMD [\"./uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8090\"] EOF 3.1 Build and run application $ docker build -t studentbook . $ docker run -p 8090:8090 studentbook 4.0 Create a Single Stage Buildah File cat <<'EOF' >>single-buildah.sh #!/usr/bin/env bash set -euo pipefail # Create a new container from the python:3.8-slim image container=$(buildah from python:3.8-slim) # Install dependencies buildah run \"$container\" -- pip install fastapi uvicorn # Copy the application code buildah copy \"$container\" . /app # Set the working directory buildah config --workingdir /app \"$container\" # Set the entrypoint buildah config --entrypoint \"uvicorn app:app --host 0.0.0.0 --port 8090\" \"$container\" # Commit the container to an image buildah commit \"$container\" studentbook # Remove the container buildah rm \"$container\" EOF sudo chmod +x single-buildah.sh ./single-buildah.sh $ podman run -p 8090:8090 studentbook 90:8090 localhost:5001/studentbook:1.0 6.0 Push student book to DockerHub # assumes you have DockerHub account set-up 6.1 Enter docker username and password (NOTE not secure to do it this way) $ docker login -u your_user_name 6.2 Tag image (if not already done from above) $ docker build -t studentbook $ docker image tag studentbook USER/studentbook:v1.0 6.3 Push Image $ docker image push USER/studentbook:v1.0 $ docker push ``` Additional Challenges \u00b6 Security Tooling Container - Building your own security tooling container can be useful. Using Hacker Container as inspiration, craft your own security tooling container that you can use for future endeavors. Some design consideration are: Conatiner should be built using a multistage process Keep the container as lean as possible Consider implementing a flag that will only add tools for a given a situation. For example, you could have a light, medium, and heavy option that includes more or less tooling depending on the flag. Create a multistage Buildah file - Using Buildah, create a multistage build file and deploy the studentbook app in a container.","title":"3.Image From Scratch"},{"location":"crawl/scratch/#3image-from-scratch","text":"","title":"3.Image From Scratch"},{"location":"crawl/scratch/#introduction","text":"This scenario assumes that you have completed the security verification practices in the last exercise set. If you have not done this, you should ensure you complete these steps and understand the importance of inspecting an image before using it. For this scenario, we will recreate an image container from scratch and push it to a container repository and then pull it for use locally. There are several reasons why someone might want to build a Docker image from scratch: Customization Building a container image from scratch allows you to fully customize the contents of the image to meet your specific needs. You can include only the libraries, dependencies, and applications that you need, and configure the image in exactly the way that you want. Size Building a container image from scratch can result in a smaller final image size, as you only include the components that you need and can optimize the image for your specific use case. Security Building a Container image from scratch allows you to start with a minimal base image and add only the components that you need, which can help to reduce the attack surface and improve security Control Building a Docker image from scratch gives you complete control over the contents and configuration of the image, which can be useful if you have specific requirements or constraints. Learning Building a Docker image from scratch can also be a useful learning exercise, as it allows you to understand the process of creating a Docker image and the various components that go into it.","title":"Introduction"},{"location":"crawl/scratch/#single-vs-multi-stage-image-building","text":"We will also explore building single-stage and multistage container images. A single-stage container build is a process in which all the necessary steps to create a container image are performed in a single build stage. This can include building the application, installing dependencies, and packaging the final image. On the other hand, a multistage container build involves creating the container image in multiple stages. Each stage in the build process includes only the packages and libraries necessary for that stage rather than the entire build environment. The final stage of the build process creates the final container image, which only includes the packages and libraries needed for the application. There are several reasons why it can be beneficial to build containers in multiple stages: Smaller image size Building a container in multiple stages allows you to create smaller final images. This is because each stage in the build process only includes the packages and libraries necessary for that stage, rather than the entire build environment. This can make it easier to deploy and manage the container, as well as reduce the risk of security vulnerabilities. Improved Security Building a container in multiple stages can improve security by reducing the attack surface of the final image. This is because the final image only includes the packages and libraries necessary for the application, rather than the entire build environment. Simplified maintenance Building a container in multiple stages can make it easier to maintain the container over time. This is because you can update or rebuild individual stages of the build process without affecting the entire container. Improved build efficiency Building a container in multiple stages can improve build efficiency by allowing you to reuse common build steps across multiple containers. This can save time and reduce the complexity of the build process. Overall, building containers in multiple stages can provide a number of benefits, including smaller image size, improved security, simplified maintenance, and improved build efficiency.","title":"Single vs multi stage image building"},{"location":"crawl/scratch/#supplementary-learning-material","text":"","title":"Supplementary Learning Material"},{"location":"crawl/scratch/#additional-links","text":"Podman for Devops Chapter 6 - https://learning.oreilly.com/library/view/podman-for-devops/9781803248233/B17908_06_epub.xhtml#_idParaDest-131 Buildah as a cli tool - https://hackernoon.com/ Getting started Buildah - <(https://opensource.com/article/18/6/getting-started-buildah> Buildah and OCI Images - https://www.linode.com/docs/guides/using-buildah-oci-images/ Github buildah tutorial - https://github.com/containers/buildah/blob/main/docs/tutorials/01-intro.md) Docker Multistage - https://docs.docker.com/build/building/multi-stage Buildah vs Kaniko - https://earthly.dev/blog/docker-vs-buildah-vs-kaniko/ Distroless - https://github.com/GoogleContainerTools/distroless distroless - https://medium.com/@luke_perry_dev/dockerizing-with-distroless-f3b84ae10f3a","title":"Additional Links:"},{"location":"crawl/scratch/#scenario","text":"Install and Test Student API Application Create a Basic Dockerfile Create a Multistage DockerFile Create a Single Stage Buildah File Creat a MultiStage Buildah File Push image to Dockerhub Solutions Solution 1.Install and Test Student API Application 1.1 Set-up environment $ mkdir student $ cd student $ sudo apt-get install python3-venv $ python3 -m venv venv $ source venv/bin/activate $ pip install fastapi uvicorn $ sudo apt-get install jq 1.2 Create Application cat <<'EOF' >>app.py # from fastapi import FastAPI, HTTPException from typing import Optional from pydantic import BaseModel app = FastAPI() students = [ {'name': 'Student 1', 'age': 20}, {'name': 'Student 2', 'age': 18}, {'name': 'Student 3', 'age': 16} ] class Student(BaseModel): name: str age: int @app.get('/students') def user_list(min: Optional[int] = None, max: Optional[int] = None): if min and max: filtered_students = list( filter(lambda student: max >= student['age'] >= min, students) ) return {'students': filtered_students} return {'students': students} @app.get('/students/{student_id}') def user_detail(student_id: int): student_check(student_id) return {'student': students[student_id]} @app.post('/students') def user_add(student: Student): students.append(student) return {'student': students[-1]} @app.put('/students/{student_id}') def user_update(student: Student, student_id: int): student_check(student_id) students[student_id].update(student) return {'student': students[student_id]} @app.delete('/students/{student_id}') def user_delete(student_id: int): student_check(student_id) del students[student_id] return {'students': students} def student_check(student_id): if not students[student_id]: raise HTTPException(status_code=404, detail='Student Not Found') EOF 1.3 Run application uvicorn --port 8090 app:app # Create the requirements.txt for next stages $ pip freeze > requirements.txt 1.4 Test the API # Get all students $ curl http://127.0.0.1:8090/students # Filter students based on age $ curl \"http://127.0.0.1:8090/students?min=16&max=18\" # Get single student $ curl http://127.0.0.1:8090/students/0T # Add student $ curl -X 'POST' http://127.0.0.1:8090/students -H 'Content-Type: application/json' -d '{\"name\":\"Tekton Operator\", \"age\": 99}' # update student $ curl -X 'PUT' http://127.0.0.1:8090/students/0 -H 'Content-Type: application/json' -d '{\"name\":\"Student X\", \"age\": 18}' # Delete Student $curl -X 'DELETE' http://127.0.0.1:8090/students/3 2.0 Create a Basic Dockerfile cat <<'EOF' >>Dockerfile FROM python:3.8 # Install dependencies COPY requirements.txt . RUN pip install -r requirements.txt # Copy the application code COPY . . # Run the application CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8090\"] EOF 2.1 Build and run application $ docker build -t studentbook . $ docker run -p 8090:8090 --name sb studentbook 3.0 Create a Multistage DockerFile cat <<'EOF' >>Dockerfile # Stage 1 - Build stage FROM python:3.10-slim AS build-env # Copy the application code COPY . /app WORKDIR /app # Install dependencies RUN pip install --no-cache-dir --upgrade -r requirements.txt && cp $(which uvicorn) /app # Stage 2 - Distroless container FROM gcr.io/distroless/python3 # Copy the built application from the build stage COPY --from=build-env /app /app COPY --from=build-env /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages ENV PYTHONPATH=/usr/local/lib/python3.10/site-packages WORKDIR /app # Run the application CMD [\"./uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8090\"] EOF 3.1 Build and run application $ docker build -t studentbook . $ docker run -p 8090:8090 studentbook 4.0 Create a Single Stage Buildah File cat <<'EOF' >>single-buildah.sh #!/usr/bin/env bash set -euo pipefail # Create a new container from the python:3.8-slim image container=$(buildah from python:3.8-slim) # Install dependencies buildah run \"$container\" -- pip install fastapi uvicorn # Copy the application code buildah copy \"$container\" . /app # Set the working directory buildah config --workingdir /app \"$container\" # Set the entrypoint buildah config --entrypoint \"uvicorn app:app --host 0.0.0.0 --port 8090\" \"$container\" # Commit the container to an image buildah commit \"$container\" studentbook # Remove the container buildah rm \"$container\" EOF sudo chmod +x single-buildah.sh ./single-buildah.sh $ podman run -p 8090:8090 studentbook 90:8090 localhost:5001/studentbook:1.0 6.0 Push student book to DockerHub # assumes you have DockerHub account set-up 6.1 Enter docker username and password (NOTE not secure to do it this way) $ docker login -u your_user_name 6.2 Tag image (if not already done from above) $ docker build -t studentbook $ docker image tag studentbook USER/studentbook:v1.0 6.3 Push Image $ docker image push USER/studentbook:v1.0 $ docker push ```","title":"Scenario"},{"location":"crawl/scratch/#additional-challenges","text":"Security Tooling Container - Building your own security tooling container can be useful. Using Hacker Container as inspiration, craft your own security tooling container that you can use for future endeavors. Some design consideration are: Conatiner should be built using a multistage process Keep the container as lean as possible Consider implementing a flag that will only add tools for a given a situation. For example, you could have a light, medium, and heavy option that includes more or less tooling depending on the flag. Create a multistage Buildah file - Using Buildah, create a multistage build file and deploy the studentbook app in a container.","title":"Additional Challenges"},{"location":"crawl/sign/","text":"4.Image Signing \u00b6 Introduction \u00b6 Signing container images is a crucial step in ensuring the authenticity and integrity of the images. Signing the images guarantees that they have not been tampered with or compromised in any way. This is particularly important in production environments where containers are used to run critical applications. Signing images ensures security, compliance, and trust. It also provides transparency, allowing one to see which images have been modified and by whom. Signed images can be helpful for auditing and troubleshooting purposes as well as compliance, security, and regulatory requirements such as PCI-DSS, HIPAA, and SOC2. Additionally, it helps to establish trust in the images for organizations that share images with others, such as open-source projects or commercial vendors. Although signing images can help to ensure the authenticity and integrity of the images, it is not a replacement for other security measures such as vulnerability scanning or runtime security. There are several options for signing containers, each with its own advantages and use cases. The best option depends on the specific use case and requirements, and you should carefully evaluate the options available to find the best solution for your needs. Signing Options Digital Signatures, such as X.509 certificates, are used to sign the container and verify its authenticity. This method is considered to be one of the most secure options for container signing, as it provides a high level of trust and is widely used in enterprise environments. Hash-Based Signatures, such as SHA-256, create a unique signature for the container, which can be verified using the same function. This method is considered to be less secure than digital signatures, but it is still a reliable way to ensure the authenticity of the container image. Notary is a centralized service that allows users to sign and verify the authenticity of containers. It uses digital signatures, such as X.509 certificates, to sign the container and verify its authenticity. It also provides a centralized repository of signed containers and allows for the management of multiple signing keys. TUF (The Update Framework) is an open-source framework for securing software update systems, which can be used to sign and verify container images. It provides a flexible and secure method for managing the distribution of software updates. In-Transit encryption, using transport layer security (TLS) or other encryption method to encrypt the container image during transit. This method provides an additional layer of security for container images in transit, but it does not provide the same level of trust as digital signatures or hash-based signatures. CoSign is an open-source tool for signing and verifying container images, it uses hash-based signatures, such as SHA-256, to create a unique signature for the container. The tool allows for the management of multiple signing keys, and provides a command line interface for signing and verifying images. It's important to note that cosign only works with container images in the OCI format, it's not compatible with other container runtimes like Docker. GPG (GNU Privacy Guard) is a tool that allows you to sign and verify the authenticity of files. It can be used to sign container images before pushing them to a registry, and then use GPG to verify the signature on the image when it is pulled from the registry. This method provides authenticity of the container image, but it will not provide the same level of security as using a centralized service like Notary, which also allows for the management of multiple signing keys. Image signing with GCP, Google Cloud Platform has its own image signing service which allow you to sign container images and validate them on Google Kubernetes Engine (GKE) clusters. Image signing with AWS: AWS also provide a service called Elastic Container Registry (ECR) Image Scanning which allows you to scan container images for vulnerabilities and malware. For our purposes we will use Cosign, but most of the principles introduced are transferable to GPG or other mechanisms. Supplementary Learning Material \u00b6 Install cosign - https://docs.sigstore.dev/cosign/installation/ Using cosign - https://faun.pub/signing-container-images-using-cosign-7df8f61456ad Using cosign for image signing - https://github.com/sigstore/cosign Install yq wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq &&\\ chmod +x /usr/bin/yq https://github.com/mikefarah/yq Scenario \u00b6 For this sceanrio, we will generate a local signing key and use it to sign images. of course, in the real world private keys would be stored in a much more secure manner. Install Local Registry (no Authentication) Sign Image with Cosign and verify signature Sign Image and Verify signature Generate, sign, and attach SBOM Solution Solution 1.0 Install local container registry (no authentication) 1.1 Install/start registry $ docker run -d -p 5001:5001 --restart always --name registry registry:2 1.2 Build and tag image $ docker build -t studentbook . $ docker image tag studentbook localhost:5001/studentbook:1.0 1.3 Push image to the local registry $ docker push localhost:5001/studentbook:1.0 1.4 Inspect the image $ skopeo copy --tls-verify=false docker://localhost:5001/studentbook oci-archive:stud.tar $ skopeo inspect --tls-verify=false docker://localhost:5001/studentbook 1.5 Run image from local registry $ docker pull localhost:5001/studentbook:1.0 $ docker run -p 8090 2.0 Sign image with cosign 2.1 Install cosign # binary $ wget \"https://github.com/sigstore/cosign/releases/download/v1.6.0/cosign-linux-amd64\" $ mv cosign-linux-amd64 /usr/local/bin/cosign $ chmod +x /usr/local/bin/cosign # rpm $ wget \"https://github.com/sigstore/cosign/releases/download/v1.6.0/cosign-1.6.0.x86_64.rpm\" $ rpm -ivh cosign-1.6.0.x86_64.rpm # dkpg $ wget \"https://github.com/sigstore/cosign/releases/download/v1.6.0/cosign_1.6.0_amd64.deb\" $ dpkg -i cosign_1.6.0_amd64.deb 2.2 Generate key pair $ cosign generate-key-pair 3.0 Sign image and verify signature $ cosign sign --key cosign.key localhost:5001/studentbook/1.0 $ cosign triangulate localhost:5001/studentbook:1.0 $ cosign verify localhost:5001/studentbook:1.0 3.0. SBOM 3.1 Generate SBOM with Syft $ syft packages localhost:5001/studentbook:1.0 --scope all-layers -o spdx > sb-image.spdx 3.2 Attatch SBOM to container package $ cosign attach sbom --sbom sb-image.spdx localhost:5001/studentbook:1.0 Additional Challenges \u00b6 Add Authentication - Add authentication to your local registry Install a and use a Vault - Store and use secrets for cosign from a vault","title":"4.Image Signing"},{"location":"crawl/sign/#4image-signing","text":"","title":"4.Image Signing"},{"location":"crawl/sign/#introduction","text":"Signing container images is a crucial step in ensuring the authenticity and integrity of the images. Signing the images guarantees that they have not been tampered with or compromised in any way. This is particularly important in production environments where containers are used to run critical applications. Signing images ensures security, compliance, and trust. It also provides transparency, allowing one to see which images have been modified and by whom. Signed images can be helpful for auditing and troubleshooting purposes as well as compliance, security, and regulatory requirements such as PCI-DSS, HIPAA, and SOC2. Additionally, it helps to establish trust in the images for organizations that share images with others, such as open-source projects or commercial vendors. Although signing images can help to ensure the authenticity and integrity of the images, it is not a replacement for other security measures such as vulnerability scanning or runtime security. There are several options for signing containers, each with its own advantages and use cases. The best option depends on the specific use case and requirements, and you should carefully evaluate the options available to find the best solution for your needs. Signing Options Digital Signatures, such as X.509 certificates, are used to sign the container and verify its authenticity. This method is considered to be one of the most secure options for container signing, as it provides a high level of trust and is widely used in enterprise environments. Hash-Based Signatures, such as SHA-256, create a unique signature for the container, which can be verified using the same function. This method is considered to be less secure than digital signatures, but it is still a reliable way to ensure the authenticity of the container image. Notary is a centralized service that allows users to sign and verify the authenticity of containers. It uses digital signatures, such as X.509 certificates, to sign the container and verify its authenticity. It also provides a centralized repository of signed containers and allows for the management of multiple signing keys. TUF (The Update Framework) is an open-source framework for securing software update systems, which can be used to sign and verify container images. It provides a flexible and secure method for managing the distribution of software updates. In-Transit encryption, using transport layer security (TLS) or other encryption method to encrypt the container image during transit. This method provides an additional layer of security for container images in transit, but it does not provide the same level of trust as digital signatures or hash-based signatures. CoSign is an open-source tool for signing and verifying container images, it uses hash-based signatures, such as SHA-256, to create a unique signature for the container. The tool allows for the management of multiple signing keys, and provides a command line interface for signing and verifying images. It's important to note that cosign only works with container images in the OCI format, it's not compatible with other container runtimes like Docker. GPG (GNU Privacy Guard) is a tool that allows you to sign and verify the authenticity of files. It can be used to sign container images before pushing them to a registry, and then use GPG to verify the signature on the image when it is pulled from the registry. This method provides authenticity of the container image, but it will not provide the same level of security as using a centralized service like Notary, which also allows for the management of multiple signing keys. Image signing with GCP, Google Cloud Platform has its own image signing service which allow you to sign container images and validate them on Google Kubernetes Engine (GKE) clusters. Image signing with AWS: AWS also provide a service called Elastic Container Registry (ECR) Image Scanning which allows you to scan container images for vulnerabilities and malware. For our purposes we will use Cosign, but most of the principles introduced are transferable to GPG or other mechanisms.","title":"Introduction"},{"location":"crawl/sign/#supplementary-learning-material","text":"Install cosign - https://docs.sigstore.dev/cosign/installation/ Using cosign - https://faun.pub/signing-container-images-using-cosign-7df8f61456ad Using cosign for image signing - https://github.com/sigstore/cosign Install yq wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq &&\\ chmod +x /usr/bin/yq https://github.com/mikefarah/yq","title":"Supplementary Learning Material"},{"location":"crawl/sign/#scenario","text":"For this sceanrio, we will generate a local signing key and use it to sign images. of course, in the real world private keys would be stored in a much more secure manner. Install Local Registry (no Authentication) Sign Image with Cosign and verify signature Sign Image and Verify signature Generate, sign, and attach SBOM Solution Solution 1.0 Install local container registry (no authentication) 1.1 Install/start registry $ docker run -d -p 5001:5001 --restart always --name registry registry:2 1.2 Build and tag image $ docker build -t studentbook . $ docker image tag studentbook localhost:5001/studentbook:1.0 1.3 Push image to the local registry $ docker push localhost:5001/studentbook:1.0 1.4 Inspect the image $ skopeo copy --tls-verify=false docker://localhost:5001/studentbook oci-archive:stud.tar $ skopeo inspect --tls-verify=false docker://localhost:5001/studentbook 1.5 Run image from local registry $ docker pull localhost:5001/studentbook:1.0 $ docker run -p 8090 2.0 Sign image with cosign 2.1 Install cosign # binary $ wget \"https://github.com/sigstore/cosign/releases/download/v1.6.0/cosign-linux-amd64\" $ mv cosign-linux-amd64 /usr/local/bin/cosign $ chmod +x /usr/local/bin/cosign # rpm $ wget \"https://github.com/sigstore/cosign/releases/download/v1.6.0/cosign-1.6.0.x86_64.rpm\" $ rpm -ivh cosign-1.6.0.x86_64.rpm # dkpg $ wget \"https://github.com/sigstore/cosign/releases/download/v1.6.0/cosign_1.6.0_amd64.deb\" $ dpkg -i cosign_1.6.0_amd64.deb 2.2 Generate key pair $ cosign generate-key-pair 3.0 Sign image and verify signature $ cosign sign --key cosign.key localhost:5001/studentbook/1.0 $ cosign triangulate localhost:5001/studentbook:1.0 $ cosign verify localhost:5001/studentbook:1.0 3.0. SBOM 3.1 Generate SBOM with Syft $ syft packages localhost:5001/studentbook:1.0 --scope all-layers -o spdx > sb-image.spdx 3.2 Attatch SBOM to container package $ cosign attach sbom --sbom sb-image.spdx localhost:5001/studentbook:1.0","title":"Scenario"},{"location":"crawl/sign/#additional-challenges","text":"Add Authentication - Add authentication to your local registry Install a and use a Vault - Store and use secrets for cosign from a vault","title":"Additional Challenges"},{"location":"explorations/","text":"Explorations \u00b6 Introduction \u00b6 Explorations are the final level of our journey. The purpose of an exploration is to to dive deeply into a topic around GitSecOps and and produce a repeatable body of work so that others can learn from your journey. Explorations can be iterative! The exploration process can be repeated as you continue to learn more about the topic and deepen your understanding. It's important to be open-minded and willing to revise your understanding as you gather new information and insights. Exploratory learning can be an iterative process, and it's okay to make mistakes along the way, as long as you learn from them How and Why Explorations work Explorations can be a powerful tool for learning because they allow people to pragmatically interact with the technology. When people are given the opportunity to explore, they can ask questions, make predictions, and test their ideas in a way that helps them construct their own knowledge and understanding. This can lead to a greater sense of ownership and engagement with the material, which can in turn enhance motivation and learning outcomes. Additionally, explorations can provide a way for people to learn from and with others, which can foster collaboration and social interaction, both of which are important for learning and development. Here is a general process for exploratory learning: Identify a topic or concept that you want to learn more about. Gather information about the topic through various sources, such as books, articles, lectures, and discussions with experts or peers. Formulate questions and hypotheses about the topic. Engage in hands-on activities or experiments to test your ideas and gather data. Analyze the data and draw conclusions based on your findings. Reflect on the learning process and what you have learned. Share your findings and insights with others. Current areas in need of exploration \u00b6 Here are some of the areas we are currently looking to explore: Using an Artifact Server Hardening Containers Kubernetes PaaS Observability with Open Telemtry Open Policy Agent (OPA) Service Mesh with Istio Spiffe and spire Knative Crossplane Requirements for publication to this site \u00b6 You do not have to aim to have your exploration published on this site. However, if you do want us to publish here, then your exploration must meet the following minimum requirements: The workshop design principles must be incorporated Exploration must use the technology stack introduced as part of this workshop (Github, Tekton, ArgoCD, Kind, kubernetes). You can add variations or show different ways of doing things, but you must include the core stack. This makes it easier for other to follow along. Security must be a core part of the exploration. To maintain repeatability, the JuiceShop, StudentBook, or bookinfo application should be used as the underlying application. . A How-to/walkthrough/tutorial guide must be included. Please copy the format of the other explorations already published Suggested Process \u00b6 Complete research into one of the explorations listed above or embark on a unique one. Using the sample proposal template as a guideline. create a README, Presentation, Miro board etc outlining your proposal. Ask some peers to review your proposal for feedback. Complete your exploration, make sure to include, howto/tutorial/walkthrough that someone can use to reproduce your exploration in a hands on manner Request for your Exploration to be added to the this page. If desired, complete a Reflection like you did for the Capstone project.","title":"Explorations"},{"location":"explorations/#explorations","text":"","title":"Explorations"},{"location":"explorations/#introduction","text":"Explorations are the final level of our journey. The purpose of an exploration is to to dive deeply into a topic around GitSecOps and and produce a repeatable body of work so that others can learn from your journey. Explorations can be iterative! The exploration process can be repeated as you continue to learn more about the topic and deepen your understanding. It's important to be open-minded and willing to revise your understanding as you gather new information and insights. Exploratory learning can be an iterative process, and it's okay to make mistakes along the way, as long as you learn from them How and Why Explorations work Explorations can be a powerful tool for learning because they allow people to pragmatically interact with the technology. When people are given the opportunity to explore, they can ask questions, make predictions, and test their ideas in a way that helps them construct their own knowledge and understanding. This can lead to a greater sense of ownership and engagement with the material, which can in turn enhance motivation and learning outcomes. Additionally, explorations can provide a way for people to learn from and with others, which can foster collaboration and social interaction, both of which are important for learning and development. Here is a general process for exploratory learning: Identify a topic or concept that you want to learn more about. Gather information about the topic through various sources, such as books, articles, lectures, and discussions with experts or peers. Formulate questions and hypotheses about the topic. Engage in hands-on activities or experiments to test your ideas and gather data. Analyze the data and draw conclusions based on your findings. Reflect on the learning process and what you have learned. Share your findings and insights with others.","title":"Introduction"},{"location":"explorations/#current-areas-in-need-of-exploration","text":"Here are some of the areas we are currently looking to explore: Using an Artifact Server Hardening Containers Kubernetes PaaS Observability with Open Telemtry Open Policy Agent (OPA) Service Mesh with Istio Spiffe and spire Knative Crossplane","title":"Current areas in need of exploration"},{"location":"explorations/#requirements-for-publication-to-this-site","text":"You do not have to aim to have your exploration published on this site. However, if you do want us to publish here, then your exploration must meet the following minimum requirements: The workshop design principles must be incorporated Exploration must use the technology stack introduced as part of this workshop (Github, Tekton, ArgoCD, Kind, kubernetes). You can add variations or show different ways of doing things, but you must include the core stack. This makes it easier for other to follow along. Security must be a core part of the exploration. To maintain repeatability, the JuiceShop, StudentBook, or bookinfo application should be used as the underlying application. . A How-to/walkthrough/tutorial guide must be included. Please copy the format of the other explorations already published","title":"Requirements for publication to this site"},{"location":"explorations/#suggested-process","text":"Complete research into one of the explorations listed above or embark on a unique one. Using the sample proposal template as a guideline. create a README, Presentation, Miro board etc outlining your proposal. Ask some peers to review your proposal for feedback. Complete your exploration, make sure to include, howto/tutorial/walkthrough that someone can use to reproduce your exploration in a hands on manner Request for your Exploration to be added to the this page. If desired, complete a Reflection like you did for the Capstone project.","title":"Suggested Process"},{"location":"explorations/proposal/","text":"Proposal Template \u00b6 Title: Exploring Kubernetes for Container Orchestration \u00b6 Purpose: The purpose of this exploration is to gain a deeper understanding of Kubernetes and how it can be used for container orchestration. \u00b6 Objectives: The objectives of this exploration are to: \u00b6 Learn the basics of Kubernetes and container orchestration Deploy a sample application using Kubernetes Experiment with different features and capabilities of Kubernetes Understand the benefits and challenges of using Kubernetes in production environments Methods: The methods that will be used for this exploration include: \u00b6 Reading and studying documentation and tutorials on Kubernetes Setting up a Kubernetes cluster and deploying sample applications Testing and experimenting with different features and capabilities of Kubernetes Participating in online forums and communities to ask questions and learn from others Timeline: The timeline for this exploration is as follows: \u00b6 Week 1: Read and study documentation and tutorials on Kubernetes (duration: 1 week) Week 2: Set up a Kubernetes cluster and deploy a sample application (duration: 1 week) Week 3: Experiment with different features and capabilities of Kubernetes (duration: 1 week) Resources: The resources that will be needed for this exploration include: \u00b6 A computer with internet access A Kubernetes cluster (this can be set up using a cloud provider or on-premises using tools such as Minikube) Sample applications to deploy on the Kubernetes cluster Expected Outcomes: The expected outcomes of this exploration are: \u00b6 A deeper understanding of Kubernetes and how it can be used for container orchestration Hands-on experience deploying applications using Kubernetes Insights into the benefits and challenges of using Kubernetes in production environments Evaluation: The exploration will be evaluated based on the following criteria: \u00b6 Depth of understanding of Kubernetes and container orchestration Successful deployment of sample applications using Kubernetes Quality of insights and reflections on the benefits and challenges of using Kubernetes in production environments Budget: The budget for this exploration is as follows: \u00b6 No budget is required for this exploration, as all necessary resources are free or can be accessed through free trials. References \u00b6 [A list of relevant references] Contact Information: \u00b6 For more information, please contact [insert contact information here]. Additional Questions to consider \u00b6 Why should we solve it? * Why is this an important problem for us to solve? * Is it a big enough problem to solve? * Should we solve it now, or later? * Provide specific data (quantitative or qualitative) wherever possible. How do you propose to solve it? * Add details of your proposed change. This can be high-level as psuedocode and sketches of the architecture, or as low-level as Protocol Buffer and interface definitions. Decide what the context of the proposal is, and write appropriately. * Be explicit about what you are optimising for in this proposed solution \u2014 for example, 'we need to hit this deadline, so we'll accept the tech debt this entails'. What other approaches did you consider? * In general, it's a red-flag if you haven't thought of a couple of different ways that you could approach the problem. * What does your proposed solution give you that these approaches don't? * What are they optimising for that isn't appropriate in this case? What could go wrong? * What risks does your proposed change entail? * How will you mitigate them? * How could this system fail, and what would be the impact if it did?","title":"Proposal Template"},{"location":"explorations/proposal/#proposal-template","text":"","title":"Proposal Template"},{"location":"explorations/proposal/#title-exploring-kubernetes-for-container-orchestration","text":"","title":"Title: Exploring Kubernetes for Container Orchestration"},{"location":"explorations/proposal/#purpose-the-purpose-of-this-exploration-is-to-gain-a-deeper-understanding-of-kubernetes-and-how-it-can-be-used-for-container-orchestration","text":"","title":"Purpose: The purpose of this exploration is to gain a deeper understanding of Kubernetes and how it can be used for container orchestration."},{"location":"explorations/proposal/#objectives-the-objectives-of-this-exploration-are-to","text":"Learn the basics of Kubernetes and container orchestration Deploy a sample application using Kubernetes Experiment with different features and capabilities of Kubernetes Understand the benefits and challenges of using Kubernetes in production environments","title":"Objectives: The objectives of this exploration are to:"},{"location":"explorations/proposal/#methods-the-methods-that-will-be-used-for-this-exploration-include","text":"Reading and studying documentation and tutorials on Kubernetes Setting up a Kubernetes cluster and deploying sample applications Testing and experimenting with different features and capabilities of Kubernetes Participating in online forums and communities to ask questions and learn from others","title":"Methods: The methods that will be used for this exploration include:"},{"location":"explorations/proposal/#timeline-the-timeline-for-this-exploration-is-as-follows","text":"Week 1: Read and study documentation and tutorials on Kubernetes (duration: 1 week) Week 2: Set up a Kubernetes cluster and deploy a sample application (duration: 1 week) Week 3: Experiment with different features and capabilities of Kubernetes (duration: 1 week)","title":"Timeline: The timeline for this exploration is as follows:"},{"location":"explorations/proposal/#resources-the-resources-that-will-be-needed-for-this-exploration-include","text":"A computer with internet access A Kubernetes cluster (this can be set up using a cloud provider or on-premises using tools such as Minikube) Sample applications to deploy on the Kubernetes cluster","title":"Resources: The resources that will be needed for this exploration include:"},{"location":"explorations/proposal/#expected-outcomes-the-expected-outcomes-of-this-exploration-are","text":"A deeper understanding of Kubernetes and how it can be used for container orchestration Hands-on experience deploying applications using Kubernetes Insights into the benefits and challenges of using Kubernetes in production environments","title":"Expected Outcomes: The expected outcomes of this exploration are:"},{"location":"explorations/proposal/#evaluation-the-exploration-will-be-evaluated-based-on-the-following-criteria","text":"Depth of understanding of Kubernetes and container orchestration Successful deployment of sample applications using Kubernetes Quality of insights and reflections on the benefits and challenges of using Kubernetes in production environments","title":"Evaluation: The exploration will be evaluated based on the following criteria:"},{"location":"explorations/proposal/#budget-the-budget-for-this-exploration-is-as-follows","text":"No budget is required for this exploration, as all necessary resources are free or can be accessed through free trials.","title":"Budget: The budget for this exploration is as follows:"},{"location":"explorations/proposal/#references","text":"[A list of relevant references]","title":"References"},{"location":"explorations/proposal/#contact-information","text":"For more information, please contact [insert contact information here].","title":"Contact Information:"},{"location":"explorations/proposal/#additional-questions-to-consider","text":"Why should we solve it? * Why is this an important problem for us to solve? * Is it a big enough problem to solve? * Should we solve it now, or later? * Provide specific data (quantitative or qualitative) wherever possible. How do you propose to solve it? * Add details of your proposed change. This can be high-level as psuedocode and sketches of the architecture, or as low-level as Protocol Buffer and interface definitions. Decide what the context of the proposal is, and write appropriately. * Be explicit about what you are optimising for in this proposed solution \u2014 for example, 'we need to hit this deadline, so we'll accept the tech debt this entails'. What other approaches did you consider? * In general, it's a red-flag if you haven't thought of a couple of different ways that you could approach the problem. * What does your proposed solution give you that these approaches don't? * What are they optimising for that isn't appropriate in this case? What could go wrong? * What risks does your proposed change entail? * How will you mitigate them? * How could this system fail, and what would be the impact if it did?","title":"Additional Questions to consider"},{"location":"fly/","text":"Fly(WIP) \u00b6 Introduction \u00b6 This section looks at the role Kubernetes and everything else we have done up to this point can play a role in ensuring the security and efficacy of our release management process. A release management process typically includes multiple environments: development, testing, staging, and production. Security testing should be performed in each environment of the release management process to ensure that vulnerabilities are identified and addressed before the software is deployed to production. The specific types of security testing that are performed in each environment may vary, but some approaches we will explore are depicted below: Production +---------------+ | +--------+ | +--|-->| Green | | -Automated +---------------+ +---------------+ +---------------+ | | +--------+ | -SAST/DAST/IAST | Development |---->| Testing* |---->| Staging |--| | | -Regression +---------------+ +---------------+ +---------------+ | | +--------+ | -Runtime Controls -SAST -DAST/SAST -SAST +--|-->| Blue | | -SRE Driven -Automated -Chaos Engineering -Automated | +--------+ | -Test under Stress -QA Driven +---------------+ -Expert Driven *Suggest dedicated environment for security testing NOT shared with integration, performance, or non-security Chaos engineering Many recommend not doing security testing in production. However, at the very least non, intrusive security testing should be done even in production. Indeed, getting the exact parity of a production environment to staging is virtually impossible. There will always be some issue or configuration that might get introduced during deployment to production (especially if the process still needs to be fully automated). However, an even better approach is to use a blue/green deployment method in production and complete a deeper and more intrusive security testing regime in production than you normally could with a single production environment. How Blue/Green deployment can facilitate security testing in production Blue/green deployment is a technique that can aid the security testing process by allowing teams to test changes and new releases in a staging environment before deploying them to production. This allows teams to test and validate changes in a non-production environment, which can help to identify and address any security issues before they are deployed to production. Here's how blue/green deployment can aid the security testing process: Isolation - By deploying changes to a separate environment, teams can isolate the testing process from production and prevent any unintended consequences from occurring. Controlled Rollback - Blue/green deployment allows teams to quickly roll back to the previous version in case of any issues, this ensures that the system is not affected by any security issues, and the teams can fix it before rolling out to production. *esting and Validation - By deploying changes to a staging environment, teams can test and validate changes in a realistic environment, including security testing, which can help to identify and address any security issues before they are deployed to production. Compliance - Blue/green deployment allows teams to test changes in a non-production environment, which can help to ensure that the changes are compliant with security standards and best practices before they are deployed to production. By using blue/green deployment, teams can ensure that changes are thoroughly tested and validated in a non-production environment before they are deployed to production, which can help to improve the security of the system and reduce the risk of security breaches. Here are some more of the scenarios we will explore Violent Pentesting - Use common tooling to complete a small pentest exercise Runtime Security - Implement basic runtime security for Kubernetes Security Chaos Engineering - Small example of chaos engineering Security testing in production - Run through security testing in a blue/green deployment scenario","title":"Fly(WIP)"},{"location":"fly/#flywip","text":"","title":"Fly(WIP)"},{"location":"fly/#introduction","text":"This section looks at the role Kubernetes and everything else we have done up to this point can play a role in ensuring the security and efficacy of our release management process. A release management process typically includes multiple environments: development, testing, staging, and production. Security testing should be performed in each environment of the release management process to ensure that vulnerabilities are identified and addressed before the software is deployed to production. The specific types of security testing that are performed in each environment may vary, but some approaches we will explore are depicted below: Production +---------------+ | +--------+ | +--|-->| Green | | -Automated +---------------+ +---------------+ +---------------+ | | +--------+ | -SAST/DAST/IAST | Development |---->| Testing* |---->| Staging |--| | | -Regression +---------------+ +---------------+ +---------------+ | | +--------+ | -Runtime Controls -SAST -DAST/SAST -SAST +--|-->| Blue | | -SRE Driven -Automated -Chaos Engineering -Automated | +--------+ | -Test under Stress -QA Driven +---------------+ -Expert Driven *Suggest dedicated environment for security testing NOT shared with integration, performance, or non-security Chaos engineering Many recommend not doing security testing in production. However, at the very least non, intrusive security testing should be done even in production. Indeed, getting the exact parity of a production environment to staging is virtually impossible. There will always be some issue or configuration that might get introduced during deployment to production (especially if the process still needs to be fully automated). However, an even better approach is to use a blue/green deployment method in production and complete a deeper and more intrusive security testing regime in production than you normally could with a single production environment. How Blue/Green deployment can facilitate security testing in production Blue/green deployment is a technique that can aid the security testing process by allowing teams to test changes and new releases in a staging environment before deploying them to production. This allows teams to test and validate changes in a non-production environment, which can help to identify and address any security issues before they are deployed to production. Here's how blue/green deployment can aid the security testing process: Isolation - By deploying changes to a separate environment, teams can isolate the testing process from production and prevent any unintended consequences from occurring. Controlled Rollback - Blue/green deployment allows teams to quickly roll back to the previous version in case of any issues, this ensures that the system is not affected by any security issues, and the teams can fix it before rolling out to production. *esting and Validation - By deploying changes to a staging environment, teams can test and validate changes in a realistic environment, including security testing, which can help to identify and address any security issues before they are deployed to production. Compliance - Blue/green deployment allows teams to test changes in a non-production environment, which can help to ensure that the changes are compliant with security standards and best practices before they are deployed to production. By using blue/green deployment, teams can ensure that changes are thoroughly tested and validated in a non-production environment before they are deployed to production, which can help to improve the security of the system and reduce the risk of security breaches. Here are some more of the scenarios we will explore Violent Pentesting - Use common tooling to complete a small pentest exercise Runtime Security - Implement basic runtime security for Kubernetes Security Chaos Engineering - Small example of chaos engineering Security testing in production - Run through security testing in a blue/green deployment scenario","title":"Introduction"},{"location":"run/","text":"Run(WIP) \u00b6 Introduction \u00b6 In the run stage, we will explore using Pipelines as Code(PaC) to build and deploy our application to our Kubernetes cluster. PaC refers to defining the steps of a continuous integration and delivery (CI/CD) pipeline in code form, rather than using a graphical user interface (GUI) to configure the Pipeline. This allows the pipeline configuration to be stored in version control along with the rest of the application code, making it easier to track changes to the Pipeline over time and roll back to previous versions if necessary. We will also strongly emphasize automating all the security testing we have done up to this point with the CI portion of the Pipeline. +-----------------------------------------------------------------------------+ +------------------+ +---------------------+ | Establish Trust | | Verify Trust | | Maintain Trust | +-----------------------------------------------------------------------------+ +------------------+ +---------------------+ +--------------------------------------------------------+ +-----------------------------------------------+ | Static Automated Security Testing | | DAST & Runtime Security | +-----------------------------------------------------+>-+ +----------------------------------+------------+ | | | | | | | | | | | | +-------+------------------+--------------------+------------|---------------------+-------------------|-------+ | | Vulnerability | Testing | | | | | +-------+---+--------------+--+-----------------+--+---------|-------+-------------+---+---------------|---+---+ | | | | | | | | | | | | v v v v v v v v v v v v +----------------------------------------------------------------------------+ +-------------------+ +-------------------+ | Tekton | | ArgoCD | | Observability | | +---------------+ +---------------+ +---------------+ +---------------+ | | +---------------+ | | +---------------+ | | | Code | | Build | | Test | | Artifacts | | | | Deploy | | | | Operate | | | +---------------+ +---------------+ +---------------+ +---------------+ | | +---------------+ | | +---------------+ | | | | | | | +----------------------------------------------------------------------------+ +-------------------+ +-------------------+ ^ ^ | | +-------+-------+ +-------+-------+ | Code App Repo | | Config Repo | +---------------+ +---------------+ We will use Tekton as the CI for Pipelines as Code for our purposes. Tekton is an open-source project for creating cloud-native continuous integration and delivery (CI/CD) pipelines. It is designed to be easy to use and flexible, focusing on building and deploying containerized applications. However, for the most part, everything we do could be ported to the CI tool of your choice. Some of the benefits of using Tekton for building CI/CD pipelines include: Cloud-native Tekton is designed to be run in a cloud environment, making it easy to build and deploy containerized applications to the cloud. Customizable Tekton provides a set of modular building blocks that can be combined and customized to create a CI/CD pipeline that meets the specific needs of your organization. Kubernetes native Tekton is built on top of Kubernetes, making it easy to use with other Kubernetes-based tools and platforms. Flexible Tekton can be used with a variety of different tools and technologies, including container registry platforms, source control systems, and build and deployment tools. Scalable Tekton is designed to scale to meet the needs of large organizations, with the ability to run pipelines concurrently and handle large numbers of builds and deployments. Security Automation Tekton pipelines can be configured to include security scanning as a step in the pipeline process. This can be done by including tasks that run security scanning tools and services, such as Aqua Security, Snyk, and Anchore, as part of the pipeline. Here are the scenarios we will explore:: Tasks - Through a simple Hello World we learn the fundamentals of Tekton Pipelines - Expand on our use of Tekton and build an end-to-end build pipeline for the StudentBook application. Incorporating Security Testing - Add and automate the security testing we completed in the Crawl and Walk sections. Enforcing Integrity with Tekton - Using Tekton to enforce security through out the build and deploy phases.","title":"Run(WIP)"},{"location":"run/#runwip","text":"","title":"Run(WIP)"},{"location":"run/#introduction","text":"In the run stage, we will explore using Pipelines as Code(PaC) to build and deploy our application to our Kubernetes cluster. PaC refers to defining the steps of a continuous integration and delivery (CI/CD) pipeline in code form, rather than using a graphical user interface (GUI) to configure the Pipeline. This allows the pipeline configuration to be stored in version control along with the rest of the application code, making it easier to track changes to the Pipeline over time and roll back to previous versions if necessary. We will also strongly emphasize automating all the security testing we have done up to this point with the CI portion of the Pipeline. +-----------------------------------------------------------------------------+ +------------------+ +---------------------+ | Establish Trust | | Verify Trust | | Maintain Trust | +-----------------------------------------------------------------------------+ +------------------+ +---------------------+ +--------------------------------------------------------+ +-----------------------------------------------+ | Static Automated Security Testing | | DAST & Runtime Security | +-----------------------------------------------------+>-+ +----------------------------------+------------+ | | | | | | | | | | | | +-------+------------------+--------------------+------------|---------------------+-------------------|-------+ | | Vulnerability | Testing | | | | | +-------+---+--------------+--+-----------------+--+---------|-------+-------------+---+---------------|---+---+ | | | | | | | | | | | | v v v v v v v v v v v v +----------------------------------------------------------------------------+ +-------------------+ +-------------------+ | Tekton | | ArgoCD | | Observability | | +---------------+ +---------------+ +---------------+ +---------------+ | | +---------------+ | | +---------------+ | | | Code | | Build | | Test | | Artifacts | | | | Deploy | | | | Operate | | | +---------------+ +---------------+ +---------------+ +---------------+ | | +---------------+ | | +---------------+ | | | | | | | +----------------------------------------------------------------------------+ +-------------------+ +-------------------+ ^ ^ | | +-------+-------+ +-------+-------+ | Code App Repo | | Config Repo | +---------------+ +---------------+ We will use Tekton as the CI for Pipelines as Code for our purposes. Tekton is an open-source project for creating cloud-native continuous integration and delivery (CI/CD) pipelines. It is designed to be easy to use and flexible, focusing on building and deploying containerized applications. However, for the most part, everything we do could be ported to the CI tool of your choice. Some of the benefits of using Tekton for building CI/CD pipelines include: Cloud-native Tekton is designed to be run in a cloud environment, making it easy to build and deploy containerized applications to the cloud. Customizable Tekton provides a set of modular building blocks that can be combined and customized to create a CI/CD pipeline that meets the specific needs of your organization. Kubernetes native Tekton is built on top of Kubernetes, making it easy to use with other Kubernetes-based tools and platforms. Flexible Tekton can be used with a variety of different tools and technologies, including container registry platforms, source control systems, and build and deployment tools. Scalable Tekton is designed to scale to meet the needs of large organizations, with the ability to run pipelines concurrently and handle large numbers of builds and deployments. Security Automation Tekton pipelines can be configured to include security scanning as a step in the pipeline process. This can be done by including tasks that run security scanning tools and services, such as Aqua Security, Snyk, and Anchore, as part of the pipeline. Here are the scenarios we will explore:: Tasks - Through a simple Hello World we learn the fundamentals of Tekton Pipelines - Expand on our use of Tekton and build an end-to-end build pipeline for the StudentBook application. Incorporating Security Testing - Add and automate the security testing we completed in the Crawl and Walk sections. Enforcing Integrity with Tekton - Using Tekton to enforce security through out the build and deploy phases.","title":"Introduction"},{"location":"walk/","text":"Kubernetes \u00b6 Introduction \u00b6 GiSectOps in Kubernetes refers to a method of managing infrastructure and applications by using Git as the single source of truth for configuration and deployment. It is a way to automate and simplify the management of Kubernetes clusters by using Git as the central place to store, version, and track all the configuration files for the application and infrastructure. In this scenario, we will work towards building the following environment. +-----------------------------------------------------------------------+ | KUBERNETES CLUSTER | +-----------{#}-+ | +--------------------+ | | GIT REPO | | | ArgoCD Namespace | +------------------{#}-+ | | +-----------+ | Monitors | | +--------------+ | | APP Namespace | | | | App Code | |<----------|---|--| ArgoCD | | Update/Apply | +----------------+ | | | +-----------+ | | | +--------------+ +-------+ +----->| | StudentBook | | | | | | | | | | | +----------------+ | | | +-----------+ | | | +--------------+ | | | | | | | | Manifests | |<----------|---|--| ArgoCD Image | | +-----{#}-----+ +----------------------+ | | +-----------+ | Rewrite | | | Updater | | | Kyverno | | | | Tag | | +--------------+ | | Enforcement | | +---------------+ | | | | +-------------+ | | +--------------------+ | | | | +-----------------------------------------------------------------------+ +---------{#}-+ | | CI | | Monitors {#} = Securtiy Inspection | +--------+ | | | | Tekton | | v | +--------+ | +-------------------------------------{#}---+ | | Publish | REGISTRY | | +--------+ | Image | +-----------+ +-----------+ +-----------+ | | | Kaniko |--|----------->| | Harbor | | Local | | DockerHub | | | +--------+ | | +-----------+ +-----------+ +-----------+ | | | | | +-------------+ +-------------------------------------------+ With GitSecOps for Kubernetes, developers can set up an automated pipeline that periodically checks for updates to container images in a specified container registry (e.g., Docker Hub, Quay) and automatically updates the images in the cluster using tools such as Kaniko. This ensures that applications are always running on the latest version of the container images, which can include security patches and other important updates. Further, using tools like Kyverno, one can define and enforce policies across the entire cluster, including policies for ensuring the use of signed images. Here is a summary of some of the ways that GitSecOps can be used to improve the security of applications deployed on Kubernetes: Secure the application code Use Git and code review processes to ensure that the application code is secure and follows best practices. This can include using static code analysis tools to catch security vulnerabilities, as well as implementing access controls to manage access to the codebase. Use Git for configuration management Use Git to manage the configuration of your Kubernetes cluster and applications. This can help ensure that changes to the configuration are tracked and can be easily rolled back if necessary. Use Git for continuous integration and delivery Use Git and CI/CD tools to automate the build, test, and deployment process for your applications. This can help ensure that applications are deployed quickly and safely, while also making it easier to roll back deployments if necessary. Monitor and track security vulnerabilities Use Git and security tools to track and monitor security vulnerabilities in your applications and infrastructure. This can help you identify and fix vulnerabilities before they are exploited. Scenarios Explored \u00b6 K8s, Kind and Security Scanning - Set-up a local Kubernetes development clusters with Kind and deploy images using Kaniko. Build Images with Kaniko - Using Kaniko to build images without Docker Daemon Declarative Continuous Delivery - Using Declarative Continuous Delivery to manage deployment of an application.. Image Verification in K8s - Detecting and enforcing the use of signed images in K8s.","title":"Kubernetes"},{"location":"walk/#kubernetes","text":"","title":"Kubernetes"},{"location":"walk/#introduction","text":"GiSectOps in Kubernetes refers to a method of managing infrastructure and applications by using Git as the single source of truth for configuration and deployment. It is a way to automate and simplify the management of Kubernetes clusters by using Git as the central place to store, version, and track all the configuration files for the application and infrastructure. In this scenario, we will work towards building the following environment. +-----------------------------------------------------------------------+ | KUBERNETES CLUSTER | +-----------{#}-+ | +--------------------+ | | GIT REPO | | | ArgoCD Namespace | +------------------{#}-+ | | +-----------+ | Monitors | | +--------------+ | | APP Namespace | | | | App Code | |<----------|---|--| ArgoCD | | Update/Apply | +----------------+ | | | +-----------+ | | | +--------------+ +-------+ +----->| | StudentBook | | | | | | | | | | | +----------------+ | | | +-----------+ | | | +--------------+ | | | | | | | | Manifests | |<----------|---|--| ArgoCD Image | | +-----{#}-----+ +----------------------+ | | +-----------+ | Rewrite | | | Updater | | | Kyverno | | | | Tag | | +--------------+ | | Enforcement | | +---------------+ | | | | +-------------+ | | +--------------------+ | | | | +-----------------------------------------------------------------------+ +---------{#}-+ | | CI | | Monitors {#} = Securtiy Inspection | +--------+ | | | | Tekton | | v | +--------+ | +-------------------------------------{#}---+ | | Publish | REGISTRY | | +--------+ | Image | +-----------+ +-----------+ +-----------+ | | | Kaniko |--|----------->| | Harbor | | Local | | DockerHub | | | +--------+ | | +-----------+ +-----------+ +-----------+ | | | | | +-------------+ +-------------------------------------------+ With GitSecOps for Kubernetes, developers can set up an automated pipeline that periodically checks for updates to container images in a specified container registry (e.g., Docker Hub, Quay) and automatically updates the images in the cluster using tools such as Kaniko. This ensures that applications are always running on the latest version of the container images, which can include security patches and other important updates. Further, using tools like Kyverno, one can define and enforce policies across the entire cluster, including policies for ensuring the use of signed images. Here is a summary of some of the ways that GitSecOps can be used to improve the security of applications deployed on Kubernetes: Secure the application code Use Git and code review processes to ensure that the application code is secure and follows best practices. This can include using static code analysis tools to catch security vulnerabilities, as well as implementing access controls to manage access to the codebase. Use Git for configuration management Use Git to manage the configuration of your Kubernetes cluster and applications. This can help ensure that changes to the configuration are tracked and can be easily rolled back if necessary. Use Git for continuous integration and delivery Use Git and CI/CD tools to automate the build, test, and deployment process for your applications. This can help ensure that applications are deployed quickly and safely, while also making it easier to roll back deployments if necessary. Monitor and track security vulnerabilities Use Git and security tools to track and monitor security vulnerabilities in your applications and infrastructure. This can help you identify and fix vulnerabilities before they are exploited.","title":"Introduction"},{"location":"walk/#scenarios-explored","text":"K8s, Kind and Security Scanning - Set-up a local Kubernetes development clusters with Kind and deploy images using Kaniko. Build Images with Kaniko - Using Kaniko to build images without Docker Daemon Declarative Continuous Delivery - Using Declarative Continuous Delivery to manage deployment of an application.. Image Verification in K8s - Detecting and enforcing the use of signed images in K8s.","title":"Scenarios Explored"},{"location":"walk/cd/","text":"3.Declarative Continuous Delivery \u00b6 Introduction \u00b6 Declarative Continuous Delivery (CD) is a method of automating software releases that emphasize the use of declarative configuration files to define the system's desired state. It is a way to specify how the software should be deployed rather than manually executing a series of commands to accomplish the deployment. The declaration can include what software version should be deployed, the target environment, and any other relevant information. These configuration files are then used to deploy and update the software consistently and predictably automatically. One of the key advantages of a declarative CD is that it allows for version control of the configuration files, which enables teams to track changes and roll back to previous versions if necessary. It also allows for easier collaboration among team members and makes it easier to automate the deployment process. Declarative Continuous Delivery (CD) can help improve security in several ways: Consistency and predictability By using declarative configuration files to define the desired state of the system, declarative CD ensures that software is deployed in a consistent and predictable manner. This makes it easier to detect and address any security issues that may arise. Version Control Declarative CD enables teams to version control their configuration files, which allows them to track changes and roll back to previous versions if necessary. This makes it easy to identify and fix any security issues that may have been introduced with a new release. Automation Declarative CD automates the deployment process, which reduces the risk of human error and makes it easier to detect and address security issues in a timely manner. Isolation Declarative CD often used in conjunction with containers and container orchestration tools such as Kubernetes, which provide a consistent and predictable environment for deploying software. This isolation helps to mitigate the risk of security breaches. Compliance Declarative CD can be integrated with security scanning tools to ensure that the deployed software is compliant with security standards and best practices. This helps to ensure that the software is secure and that any vulnerabilities are identified and addressed in a timely manner. For our purposes, we will use ArgoCD and ArgoCD Image Updater for CD and image updates. Argo CD is a GitOps-based continuous delivery tool that allows developers to manage and deploy applications in a Kubernetes cluster declaratively. The Image Updater feature in Argo CD allows developers to quickly update container images in their deployed applications without manually updating the image version in the configuration files. The Image Updater feature works by periodically checking for updates to container images in a specified container registry (e.g. Docker Hub, Quay) and comparing them to the images currently deployed in the cluster. If an update is available, the Image Updater will automatically update the image in the cluster and update the configuration files in the Git repository to reflect the new image version. Supplementary Learning Material \u00b6 Additional Links: \u00b6 ArgoCD - https://argo-cd.readthedocs.io/en/stable/ Harbor - https://goharbor.io/ kind and Ingress - https://kind.sigs.k8s.io/docs/user/ingress/ Local Harbor Install https://serverascode.com/2020/04/28/local-harbor-install.html ArgoCD Install - https://tanzu.vmware.com/developer/guides/argocd-gs/ Triy & Harbor - https://artifacthub.io/packages/helm/trivy-operator/harbor-scanner-trivy/0.28.0 Kubernetes Namespaces https://www.aquasec.com/cloud-native-academy/kubernetes-101/kubernetes-namespace/ Kubens and Kubectx for nmespace/cluster management - https://github.com/ahmetb/kubectx Harbor and Helm - https://itnext.io/need-a-container-image-registry-and-helm-chart-repository-go-harbor-b0c0d4eafd3b Harbor and Helm https://mannimal.blog/2019/07/31/using-harbor-and-kubeapps-to-serve-custom-helm-charts/ Scenario \u00b6 Create Kind cluster with NGNIX Support Install Harbor (make sure to enable support for Trivy) Create new project called juice (easiest to do via the GUI) Use Docker Client to push juiceshop to Harbor Try Pulling Juice Image from Harbor Upload Juice Helm Chart to Harbor Install ArgoCD and Argo Cli Create an Argo Application Deploy StudentBook via ArgoCD Solution 1.0 Create Kind cluster with NGNIX Support 1.1 Install Kind with Ingress Support $ cat <<EOF | kind create cluster --name juice --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF 1.2 Install Ingress 1.21. If using Ngnix $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml 1.2.2 If using Contour $ kubectl apply -f https://projectcontour.io/quickstart/contour.yaml $ kubectl patch daemonsets -n projectcontour envoy -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"ingress-ready\":\"true\"},\"tolerations\":[{\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Equal\",\"effect\":\"NoSchedule\"}]}}}}' 2.0 Install Harbor (make sure to enable support for Trivy) 2.1 Add Harbor Helm repo and install $ helm repo add harbor https://helm.goharbor.io # Install in separate namespace $ helm install local-harbor harbor/harbor \\ --create-namespace \\ --namespace harbor \\ --set clair.enabled=false \\ --set trivy.enabled=true # Or install in same namespace $ helm install local-harbor harbor/harbor \\ --set clair.enabled=false \\ --set trivy.enabled=true 2.2 Accessing the site # Final configd - Add pointer 127.0.0.1 core.harbor.domain /etc/hosts - Browse https://core.harbor.domain Must access via https or login will fail - login: admin password: Harbor12345 3.0 Create a new project via the gui 1. Add a robot account 2. Set Policy to scan on push and prevent deployment of vulnerable images 3. Create project called \"juice\" 4.0 Use Docker Client to push Juiceshop to Harbor $ docker login username (will prompt for password ) $ docker pull bkimminich/juice-shop $ docker tag bkimminich/juice-shop core.harbor.domain/juice/juice:v1 $ docker push core.harbor.domain/[projectname]/juice:v1 5.0 Try Pulling Juice Image from Harbor $ docker pull core.harbor.domain/redsmith/juice:v1 Error response from daemon: unknown: current image with 52 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Low\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE allowlist. 6.0 Upload Juice Helm Chart to Harbor 6.1 Fetch Helm Chart $ helm fetch juice/juice-shop 6.2 Upload via GUI -Upload via GUI 7.0 Install ArgoCD and Argo Cli 7.1 Install ArgoCD $ kubectl create namespace argocd $ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml $ watch kubectl get pods -n argocd $ kubectl port-forward svc/argocd-server -n argocd 8080:443 7.2 Install ArgoCD Cli # Download the binary $ curl -sLO https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 # Make binary executable $ chmod +x argocd-linux-amd64 # Move binary to path $ sudo mv ./argocd-linux-amd64 /usr/local/bin/argo # Test installation $ argo version --short 7.3 Login and Change password $ argocd login localhost:8080 Note: Again, as with the UI, you will need to accept the server certificate error. ## To get the password $ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d $ argocd account update-password kubectl port-forward svc/argocd-server -n argocd 8080:443 Site now available at https://localhost:8080 7.3 Set Target K8s cluster $ argocd cluster add juice 8.0 Create an Argo Application work in progress . . . 9.0 Deploy Juiceshop via ArgoCD Work in progress . . . Additional Challenges \u00b6 Use Clair instead of Trivy - Change the default scanner to Clair or another scanning engine Add certs to Harbor - Set-up Hsrbor so that it uses proper certs and TLS Use Kaniko - Build and push the Juiceshop Image to Harbor with Kaniko","title":"3.Declarative Continuous Delivery"},{"location":"walk/cd/#3declarative-continuous-delivery","text":"","title":"3.Declarative Continuous Delivery"},{"location":"walk/cd/#introduction","text":"Declarative Continuous Delivery (CD) is a method of automating software releases that emphasize the use of declarative configuration files to define the system's desired state. It is a way to specify how the software should be deployed rather than manually executing a series of commands to accomplish the deployment. The declaration can include what software version should be deployed, the target environment, and any other relevant information. These configuration files are then used to deploy and update the software consistently and predictably automatically. One of the key advantages of a declarative CD is that it allows for version control of the configuration files, which enables teams to track changes and roll back to previous versions if necessary. It also allows for easier collaboration among team members and makes it easier to automate the deployment process. Declarative Continuous Delivery (CD) can help improve security in several ways: Consistency and predictability By using declarative configuration files to define the desired state of the system, declarative CD ensures that software is deployed in a consistent and predictable manner. This makes it easier to detect and address any security issues that may arise. Version Control Declarative CD enables teams to version control their configuration files, which allows them to track changes and roll back to previous versions if necessary. This makes it easy to identify and fix any security issues that may have been introduced with a new release. Automation Declarative CD automates the deployment process, which reduces the risk of human error and makes it easier to detect and address security issues in a timely manner. Isolation Declarative CD often used in conjunction with containers and container orchestration tools such as Kubernetes, which provide a consistent and predictable environment for deploying software. This isolation helps to mitigate the risk of security breaches. Compliance Declarative CD can be integrated with security scanning tools to ensure that the deployed software is compliant with security standards and best practices. This helps to ensure that the software is secure and that any vulnerabilities are identified and addressed in a timely manner. For our purposes, we will use ArgoCD and ArgoCD Image Updater for CD and image updates. Argo CD is a GitOps-based continuous delivery tool that allows developers to manage and deploy applications in a Kubernetes cluster declaratively. The Image Updater feature in Argo CD allows developers to quickly update container images in their deployed applications without manually updating the image version in the configuration files. The Image Updater feature works by periodically checking for updates to container images in a specified container registry (e.g. Docker Hub, Quay) and comparing them to the images currently deployed in the cluster. If an update is available, the Image Updater will automatically update the image in the cluster and update the configuration files in the Git repository to reflect the new image version.","title":"Introduction"},{"location":"walk/cd/#supplementary-learning-material","text":"","title":"Supplementary Learning Material"},{"location":"walk/cd/#additional-links","text":"ArgoCD - https://argo-cd.readthedocs.io/en/stable/ Harbor - https://goharbor.io/ kind and Ingress - https://kind.sigs.k8s.io/docs/user/ingress/ Local Harbor Install https://serverascode.com/2020/04/28/local-harbor-install.html ArgoCD Install - https://tanzu.vmware.com/developer/guides/argocd-gs/ Triy & Harbor - https://artifacthub.io/packages/helm/trivy-operator/harbor-scanner-trivy/0.28.0 Kubernetes Namespaces https://www.aquasec.com/cloud-native-academy/kubernetes-101/kubernetes-namespace/ Kubens and Kubectx for nmespace/cluster management - https://github.com/ahmetb/kubectx Harbor and Helm - https://itnext.io/need-a-container-image-registry-and-helm-chart-repository-go-harbor-b0c0d4eafd3b Harbor and Helm https://mannimal.blog/2019/07/31/using-harbor-and-kubeapps-to-serve-custom-helm-charts/","title":"Additional Links:"},{"location":"walk/cd/#scenario","text":"Create Kind cluster with NGNIX Support Install Harbor (make sure to enable support for Trivy) Create new project called juice (easiest to do via the GUI) Use Docker Client to push juiceshop to Harbor Try Pulling Juice Image from Harbor Upload Juice Helm Chart to Harbor Install ArgoCD and Argo Cli Create an Argo Application Deploy StudentBook via ArgoCD Solution 1.0 Create Kind cluster with NGNIX Support 1.1 Install Kind with Ingress Support $ cat <<EOF | kind create cluster --name juice --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF 1.2 Install Ingress 1.21. If using Ngnix $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml 1.2.2 If using Contour $ kubectl apply -f https://projectcontour.io/quickstart/contour.yaml $ kubectl patch daemonsets -n projectcontour envoy -p '{\"spec\":{\"template\":{\"spec\":{\"nodeSelector\":{\"ingress-ready\":\"true\"},\"tolerations\":[{\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Equal\",\"effect\":\"NoSchedule\"}]}}}}' 2.0 Install Harbor (make sure to enable support for Trivy) 2.1 Add Harbor Helm repo and install $ helm repo add harbor https://helm.goharbor.io # Install in separate namespace $ helm install local-harbor harbor/harbor \\ --create-namespace \\ --namespace harbor \\ --set clair.enabled=false \\ --set trivy.enabled=true # Or install in same namespace $ helm install local-harbor harbor/harbor \\ --set clair.enabled=false \\ --set trivy.enabled=true 2.2 Accessing the site # Final configd - Add pointer 127.0.0.1 core.harbor.domain /etc/hosts - Browse https://core.harbor.domain Must access via https or login will fail - login: admin password: Harbor12345 3.0 Create a new project via the gui 1. Add a robot account 2. Set Policy to scan on push and prevent deployment of vulnerable images 3. Create project called \"juice\" 4.0 Use Docker Client to push Juiceshop to Harbor $ docker login username (will prompt for password ) $ docker pull bkimminich/juice-shop $ docker tag bkimminich/juice-shop core.harbor.domain/juice/juice:v1 $ docker push core.harbor.domain/[projectname]/juice:v1 5.0 Try Pulling Juice Image from Harbor $ docker pull core.harbor.domain/redsmith/juice:v1 Error response from daemon: unknown: current image with 52 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Low\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE allowlist. 6.0 Upload Juice Helm Chart to Harbor 6.1 Fetch Helm Chart $ helm fetch juice/juice-shop 6.2 Upload via GUI -Upload via GUI 7.0 Install ArgoCD and Argo Cli 7.1 Install ArgoCD $ kubectl create namespace argocd $ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml $ watch kubectl get pods -n argocd $ kubectl port-forward svc/argocd-server -n argocd 8080:443 7.2 Install ArgoCD Cli # Download the binary $ curl -sLO https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 # Make binary executable $ chmod +x argocd-linux-amd64 # Move binary to path $ sudo mv ./argocd-linux-amd64 /usr/local/bin/argo # Test installation $ argo version --short 7.3 Login and Change password $ argocd login localhost:8080 Note: Again, as with the UI, you will need to accept the server certificate error. ## To get the password $ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d $ argocd account update-password kubectl port-forward svc/argocd-server -n argocd 8080:443 Site now available at https://localhost:8080 7.3 Set Target K8s cluster $ argocd cluster add juice 8.0 Create an Argo Application work in progress . . . 9.0 Deploy Juiceshop via ArgoCD Work in progress . . .","title":"Scenario"},{"location":"walk/cd/#additional-challenges","text":"Use Clair instead of Trivy - Change the default scanner to Clair or another scanning engine Add certs to Harbor - Set-up Hsrbor so that it uses proper certs and TLS Use Kaniko - Build and push the Juiceshop Image to Harbor with Kaniko","title":"Additional Challenges"},{"location":"walk/kaniko/","text":"2.Build Images with Kaniko \u00b6 Introduction \u00b6 Now that we have built a container from scratch and provisioned Kubernetes, we can deploy our application to the Kubernetes cluster using Kaniko. Kaniko is an open-source tool that allows building container images from a Dockerfile inside a container or Kubernetes cluster without needing a Docker daemon. This allows for building container images in a secure and isolated environment and eliminates the need to install or configure a Docker daemon on the build machine. Kaniko uses a Google-developed library called \"containerd\" to interact with the host filesystem and build the container images. Kaniko is particularly useful in scenarios where a Docker daemon cannot be used, such as building images in a Kubernetes cluster or on a machine that doesn\u2019t have Docker installed. Kaniko also allows for building images in a secure environment, as the build process is isolated from the host system and does not require access to the host's Docker daemon. Kaniko is a tool that is well-suited for use in the context of Kubernetes for several reasons: Can build containers in a container Kaniko is designed to build container images inside a container, rather than on the host system. This makes it suitable for use in environments where the host system does not have access to the container engine, such as in a Kubernetes cluster. Secure Kaniko uses a number of security features to ensure that the container images it builds are secure. These features include user namespaces, image signing, and image scanning. Fast Kaniko is designed to be fast and efficient, with a focus on minimizing the time it takes to build container images. This can be particularly beneficial in the context of Kubernetes, where fast deployment times are often a priority. Easy to use: Kaniko is easy to use and integrates well with a variety of tools and environments, including Kubernetes. Supplementary Learning \u00b6 Additional Links: Kind - https://kind.sigs.k8s.io/ Kubernetes - https://kubernetes.io/ Kubernetes with Kind - https://www.baeldung.com/ops/kubernetes-kind Kaniko and Docker- https://www.devopsmadness.com/kaniko_build_docker_images Kaniko and Kubernetes - https://computingforgeeks.com/build-container-images-using-kaniko-in-kubernetes/ Kaniko and container tools(local build context) - https://github.com/GoogleContainerTools/kaniko/blob/main/docs/tutorial.md Scenario Steps \u00b6 You will need to fork and clone the StudentBook repository Set-up Kubernetes Cluster with Kind Configure Kaniko Git Build Context Create the Kaniko Pod Manifest apply Kubernetes Pull and Test the Image in Docker Pull and Test the image in Kubernetes Rinse and repeat but wih a local Kaniko build context (file) Suggested Solution \u00b6 Solution 1.0 Set-up Kubernetes Cluster with Kind 1.1 Install Kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind # If you are going to use podmane then you need to add it as an external provider in your ~/.bashrc or ~/.zshrc file $ export KIND_EXPERIMENTAL_PROVIDER=podman # If you like, you can alos add an alias to the docker command to your ~/.bashrc or ~/.zshrc file alias docker='podman' # Other things you may need to do can be found here https://kind.sigs.k8s.io/docs/user/rootless/ 1.2 Create basic cluster (default name Kind) $ kind create cluster 2.0 Create the Container Registry Secret In an ideal scenario, you would be best to use a vault such as hashicorp to store variables. However, for now, we will just use system environment to store variables and explore our other options at a later date. PLEASE NOTE, secrets don this way are not really secret (not encrypted just encoded) 2.1 Option 1 $ kubectl create secret docker-registry docker --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email> 2.2 Option 2 $ cd /tmp $ mkdir pass $ cd pass $ echo -n 'https://index.docker.io/v1/' | base64 > ./server.txt $ echo -n 'your-username' > ./username.txt $ echo -n 'your-password' > ./password.txt $ echo -n 'your-email' > ./email.txt $ kubectl create secret generic regcred \\ --from-file=docker-server=./server.txt \\ --from-file=username=./username.txt \\ --from-file=docker-password=./password.txt \\ --from-file=docker-email=./email.txt 2.3 Delete pass directory if you made it $ tmp rm -rf pass v 2.3 Verify secrets $ kubectl get secret regcred --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode 3.0 Create the Kaniko Pod Manifest apply kubernetes 3.1 Git clone studentbook $ git clone https://github.com/mharrod/StudentBook.git 3.2 Apply Kaniko Deployment file apiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: [\"--dockerfile=Dockerfile\", \"--context=git://github.com/mharrod/StudentBook.git\", \"--destination=mharrod/studentbook-kaniko-repo:1.0\"] volumeMounts: - name: kaniko-secret mountPath: \"/kaniko/.docker\" volumes: - name: kaniko-secret secret: secretName: docker items: - key: .dockerconfigjson path: config.json restartPolicy: Never 3.3 check comnpletion of job $ kubectl logs kaniko -f $ kubectl get pods 4 Pull and Test the Image in Docker docker run -it mharrod/studentbook-kaniko-repo:1.0 5.0 Pull and Test the image in Kubernetes kubectl apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: name: studentbook spec: selector: matchLabels: app: studentbook replicas: 1 template: metadata: labels: app: hello spec: containers: - name: studentbook image: mharrod/studentbook-kaniko-repo:1.0 EOF 6.0 Configure Kaniko Local Build Context This is a less ideal way to do it, but shows how it can work without Github. Make sure you are working in your Studentbook repo that you cloned earlier 6.1 Prepare Local Cluster $docker ps $docker exec -it [contaniner:id] /bin/sh $ mkdir kaniko && cd kaniko $ exit $ docker cp Dockerfile container_id:/kaniko/Dockerfile 6.2 kubectl create persistent volume** kubectl apply -f - <<EOF $ kubectl create -f - <<EOF apiVersion: v1 kind: PersistentVolume metadata: name: dockerfile labels: type: local spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce storageClassName: local-storage hostPath: path: /kaniko # replace with local directory, such as \"/home/<user-name>/kaniko\" EOF 6.3 create persistent volume claim $ kubectl create -f - <<EOF kind: PersistentVolumeClaim apiVersion: v1 metadata: name: dockerfile-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: local-storage EOF 6.4 create pod and push to DockerHub $ kubectl create -f - <<EOF apiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: [\"--dockerfile=/workspace/dockerfile\", \"--context=dir://workspace\", \"--destination=mharrod/StudentBook-kaniko-local:1.0\"] volumeMounts: - name: kaniko-secret mountPath: /kaniko/.docker - name: dockerfile-storage mountPath: /workspace restartPolicy: Never volumes: - name: kaniko-secret secret: secretName: regcred items: - key: .dockerconfigjson path: config.json - name: dockerfile-storage persistentVolumeClaim: claimName: dockerfile-claim EOF 6.5 Check it worked kubectl get pods Additional Challenges \u00b6 Try podma/buildah Write a webhook that automatically makes this happen on change","title":"2.Build Images with Kaniko"},{"location":"walk/kaniko/#2build-images-with-kaniko","text":"","title":"2.Build Images with Kaniko"},{"location":"walk/kaniko/#introduction","text":"Now that we have built a container from scratch and provisioned Kubernetes, we can deploy our application to the Kubernetes cluster using Kaniko. Kaniko is an open-source tool that allows building container images from a Dockerfile inside a container or Kubernetes cluster without needing a Docker daemon. This allows for building container images in a secure and isolated environment and eliminates the need to install or configure a Docker daemon on the build machine. Kaniko uses a Google-developed library called \"containerd\" to interact with the host filesystem and build the container images. Kaniko is particularly useful in scenarios where a Docker daemon cannot be used, such as building images in a Kubernetes cluster or on a machine that doesn\u2019t have Docker installed. Kaniko also allows for building images in a secure environment, as the build process is isolated from the host system and does not require access to the host's Docker daemon. Kaniko is a tool that is well-suited for use in the context of Kubernetes for several reasons: Can build containers in a container Kaniko is designed to build container images inside a container, rather than on the host system. This makes it suitable for use in environments where the host system does not have access to the container engine, such as in a Kubernetes cluster. Secure Kaniko uses a number of security features to ensure that the container images it builds are secure. These features include user namespaces, image signing, and image scanning. Fast Kaniko is designed to be fast and efficient, with a focus on minimizing the time it takes to build container images. This can be particularly beneficial in the context of Kubernetes, where fast deployment times are often a priority. Easy to use: Kaniko is easy to use and integrates well with a variety of tools and environments, including Kubernetes.","title":"Introduction"},{"location":"walk/kaniko/#supplementary-learning","text":"Additional Links: Kind - https://kind.sigs.k8s.io/ Kubernetes - https://kubernetes.io/ Kubernetes with Kind - https://www.baeldung.com/ops/kubernetes-kind Kaniko and Docker- https://www.devopsmadness.com/kaniko_build_docker_images Kaniko and Kubernetes - https://computingforgeeks.com/build-container-images-using-kaniko-in-kubernetes/ Kaniko and container tools(local build context) - https://github.com/GoogleContainerTools/kaniko/blob/main/docs/tutorial.md","title":"Supplementary Learning"},{"location":"walk/kaniko/#scenario-steps","text":"You will need to fork and clone the StudentBook repository Set-up Kubernetes Cluster with Kind Configure Kaniko Git Build Context Create the Kaniko Pod Manifest apply Kubernetes Pull and Test the Image in Docker Pull and Test the image in Kubernetes Rinse and repeat but wih a local Kaniko build context (file)","title":"Scenario Steps"},{"location":"walk/kaniko/#suggested-solution","text":"Solution 1.0 Set-up Kubernetes Cluster with Kind 1.1 Install Kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind # If you are going to use podmane then you need to add it as an external provider in your ~/.bashrc or ~/.zshrc file $ export KIND_EXPERIMENTAL_PROVIDER=podman # If you like, you can alos add an alias to the docker command to your ~/.bashrc or ~/.zshrc file alias docker='podman' # Other things you may need to do can be found here https://kind.sigs.k8s.io/docs/user/rootless/ 1.2 Create basic cluster (default name Kind) $ kind create cluster 2.0 Create the Container Registry Secret In an ideal scenario, you would be best to use a vault such as hashicorp to store variables. However, for now, we will just use system environment to store variables and explore our other options at a later date. PLEASE NOTE, secrets don this way are not really secret (not encrypted just encoded) 2.1 Option 1 $ kubectl create secret docker-registry docker --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email> 2.2 Option 2 $ cd /tmp $ mkdir pass $ cd pass $ echo -n 'https://index.docker.io/v1/' | base64 > ./server.txt $ echo -n 'your-username' > ./username.txt $ echo -n 'your-password' > ./password.txt $ echo -n 'your-email' > ./email.txt $ kubectl create secret generic regcred \\ --from-file=docker-server=./server.txt \\ --from-file=username=./username.txt \\ --from-file=docker-password=./password.txt \\ --from-file=docker-email=./email.txt 2.3 Delete pass directory if you made it $ tmp rm -rf pass v 2.3 Verify secrets $ kubectl get secret regcred --output=\"jsonpath={.data.\\.dockerconfigjson}\" | base64 --decode 3.0 Create the Kaniko Pod Manifest apply kubernetes 3.1 Git clone studentbook $ git clone https://github.com/mharrod/StudentBook.git 3.2 Apply Kaniko Deployment file apiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: [\"--dockerfile=Dockerfile\", \"--context=git://github.com/mharrod/StudentBook.git\", \"--destination=mharrod/studentbook-kaniko-repo:1.0\"] volumeMounts: - name: kaniko-secret mountPath: \"/kaniko/.docker\" volumes: - name: kaniko-secret secret: secretName: docker items: - key: .dockerconfigjson path: config.json restartPolicy: Never 3.3 check comnpletion of job $ kubectl logs kaniko -f $ kubectl get pods 4 Pull and Test the Image in Docker docker run -it mharrod/studentbook-kaniko-repo:1.0 5.0 Pull and Test the image in Kubernetes kubectl apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: name: studentbook spec: selector: matchLabels: app: studentbook replicas: 1 template: metadata: labels: app: hello spec: containers: - name: studentbook image: mharrod/studentbook-kaniko-repo:1.0 EOF 6.0 Configure Kaniko Local Build Context This is a less ideal way to do it, but shows how it can work without Github. Make sure you are working in your Studentbook repo that you cloned earlier 6.1 Prepare Local Cluster $docker ps $docker exec -it [contaniner:id] /bin/sh $ mkdir kaniko && cd kaniko $ exit $ docker cp Dockerfile container_id:/kaniko/Dockerfile 6.2 kubectl create persistent volume** kubectl apply -f - <<EOF $ kubectl create -f - <<EOF apiVersion: v1 kind: PersistentVolume metadata: name: dockerfile labels: type: local spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce storageClassName: local-storage hostPath: path: /kaniko # replace with local directory, such as \"/home/<user-name>/kaniko\" EOF 6.3 create persistent volume claim $ kubectl create -f - <<EOF kind: PersistentVolumeClaim apiVersion: v1 metadata: name: dockerfile-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 8Gi storageClassName: local-storage EOF 6.4 create pod and push to DockerHub $ kubectl create -f - <<EOF apiVersion: v1 kind: Pod metadata: name: kaniko spec: containers: - name: kaniko image: gcr.io/kaniko-project/executor:latest args: [\"--dockerfile=/workspace/dockerfile\", \"--context=dir://workspace\", \"--destination=mharrod/StudentBook-kaniko-local:1.0\"] volumeMounts: - name: kaniko-secret mountPath: /kaniko/.docker - name: dockerfile-storage mountPath: /workspace restartPolicy: Never volumes: - name: kaniko-secret secret: secretName: regcred items: - key: .dockerconfigjson path: config.json - name: dockerfile-storage persistentVolumeClaim: claimName: dockerfile-claim EOF 6.5 Check it worked kubectl get pods","title":"Suggested Solution"},{"location":"walk/kaniko/#additional-challenges","text":"Try podma/buildah Write a webhook that automatically makes this happen on change","title":"Additional Challenges"},{"location":"walk/kubernetes/","text":"1.K8s, Kind and Security Scanning \u00b6 Introduction \u00b6 Kubernetes (often shortened to \"K8s\") is an open-source container orchestration system. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation. Kubernetes allows developers to easily deploy, scale, and manage containerized applications in a clustered environment. Kubernetes provides a set of abstractions that are used to define, deploy, and manage containerized applications. These abstractions include pods, the smallest deployable units in Kubernetes, and are used to group one or more containers. Services, which provide stable network connections to pods and Replication Controllers, ensure that the desired number of pod replicas are running at all times. Kubernetes also provides load balancing, automatic scaling, and self-healing to ensure that applications are highly available and can handle changing traffic patterns. Additionally, Kubernetes can be integrated with other tools and services, such as monitoring and logging, to provide a complete solution for managing containerized applications. To develop with Kubernetes clusters locally, we will use KIND Kubernetes IN Docker (kind). Overall, KIND is a powerful tool for local Kubernetes development because it is easy to set up and use, provides an isolated environment for testing and debugging, and integrates well with other tools. Other options, such as minikube, work equally as well. Supplementary Learning Materials \u00b6 Additional Links: Kubernetes and Kind - https://www.baeldung.com/ops/kubernetes-kind Kubernetes and Kind - https://phoenixnap.com/kb/kubernetes-kind Kubernetes cluster development with kind - https://faun.pub/creating-a-kubernetes-cluster-for-development-with-kind-189df2cb0792 Kind with podman - https://www.linkedin.com/pulse/kind-podman-ubuntu-2004lts-ludger-pottmeier Kind & K8s lab - https://cloudyuga.guru/hands_on_lab/kind-k8s Juiceshop on Kubernetes - https://networkandcode.hashnode.dev/run-owasp-juice-shop-as-a-kubernetes-service Ingress for Kubernetes - https://kind.sigs.k8s.io/docs/user/ingress/#ingress-nginx Kubescape - https://www.howtogeek.com/devops/how-to-scan-for-kubernetes-vulnerabilities-with-kubescape/ Security scanning for K8s - https://mattermost.com/blog/the-top-7-open-source-tools-for-securing-your-kubernetes-cluster/#kubeaudit Helm - https://helm.sh/docs/intro/quickstart/ Scenario \u00b6 Create a named Cluster called JuiceShop Create or copy deployment manifest and service manifest and sabe for Juiceshop (provided above) Scan Juice Manifests with Kubescape Install Juice shop with manifests Complete a security scan of the Kubernetes environment (Include: Kubesecurity bench, Kubeaudit, Kubescape) Fetch Juice-Shop Helm Chart and scan with Checkov and Helm lint Install JuiceShop Application with Helm chart Install StudentBook Using manifests Juice-Shop Manifests You are free to write these on your own or you can just copy and paste ``` cat <<'EOF'>> juice-shop-manifests.yaml kind: Deployment apiVersion: apps/v1 metadata: name: juice-shop-manifests spec: template: metadata: labels: app: juice-shop-manifests spec: containers: - name: juice-shop-manifests image: bkimminich/juice-shop selector: matchLabels: app: juice-shop-manifests --- kind: Service apiVersion: v1 metadata: name: juice-shop-manifests spec: type: NodePort selector: app: juice-shop-manifests ports: - name: http port: 8000 targetPort: 3000 --- EOF ``` Solution 1.0 Create a Cluster named Juice with NGNIX Support 1.1 Install Kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind # If you are going to use podman then you need to add it as an external provider in your ~/.bashrc or ~/.zshrc file $ export KIND_EXPERIMENTAL_PROVIDER=podman # If you like, you can alos add an alias to the docker command to your ~/.bashrc or ~/.zshrc file alias docker='podman' # Other things you may need to do can be found here https://kind.sigs.k8s.io/docs/user/rootless/ 1.2 Create Cluster default Kind cat <<EOF | kind create cluster --name studentbook --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.25.3@sha256:f52781bc0d7a19fb6c405c2af83abfeb311f130707a0e219175677e366cc45d1 kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF 2.0 Create JuiceSHop Manifests (do not deploy yet) cat <<'EOF'>> juice-shop-manifests.yaml kind: Deployment apiVersion: apps/v1 metadata: name: juice-shop-manifests spec: template: metadata: labels: app: juice-shop-manifests spec: containers: - name: juice-shop-manifests image: bkimminich/juice-shop selector: matchLabels: app: juice-shop-manifests --- kind: Service apiVersion: v1 metadata: name: juice-shop-manifests spec: type: NodePort selector: app: juice-shop-manifests ports: - name: http port: 8000 targetPort: 3000 --- EOF 3.0 Scan Juice Manifests with Kubescape curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash kubescape scan framework nsa k8s/*.yaml 4.0 Deploy Juiceshop via manifests 4.1 Create deployment kubectl create -f juice-shop-manifests.yaml 4.2 Watch service kubectl get po --watch 4.3 If want to browse by local pod IP then do following # Check service endpoints kubectl get svc juice-shop # get internal IP kubectl get no -o wide | awk '{print $6}' 4.4 If you want to browse via local host then do the following kubectl port-forward svc/juice-shop-manifests 8080:8000 5.0 Complete a security scan of the Kubernetes environment 5.1 Using Kubescape curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash # scan everything $ kubescape scan --submit --enable-host-scan --format-version v2 --verbose # scan Manifests $ kubescape scan framework nsa *.yaml # scan specific file $ kubescape scan framework nsa --exclude-namespaces kube-system,kube-public 5.2 Kubesecurity Bench kubectl apply -f - <<EOF --- apiVersion: batch/v1 kind: Job metadata: name: kube-bench spec: template: metadata: labels: app: kube-bench spec: hostPID: true containers: - name: kube-bench image: docker.io/aquasec/kube-bench:v0.6.10 command: [\"kube-bench\"] volumeMounts: - name: var-lib-etcd mountPath: /var/lib/etcd readOnly: true - name: var-lib-kubelet mountPath: /var/lib/kubelet readOnly: true - name: var-lib-kube-scheduler mountPath: /var/lib/kube-scheduler readOnly: true - name: var-lib-kube-controller-manager mountPath: /var/lib/kube-controller-manager readOnly: true - name: etc-systemd mountPath: /etc/systemd readOnly: true - name: lib-systemd mountPath: /lib/systemd/ readOnly: true - name: srv-kubernetes mountPath: /srv/kubernetes/ readOnly: true - name: etc-kubernetes mountPath: /etc/kubernetes readOnly: true # /usr/local/mount-from-host/bin is mounted to access kubectl / kubelet, for auto-detecting the Kubernetes version. # You can omit this mount if you specify --version as part of the command. - name: usr-bin mountPath: /usr/local/mount-from-host/bin readOnly: true - name: etc-cni-netd mountPath: /etc/cni/net.d/ readOnly: true - name: opt-cni-bin mountPath: /opt/cni/bin/ readOnly: true restartPolicy: Never volumes: - name: var-lib-etcd hostPath: path: \"/var/lib/etcd\" - name: var-lib-kubelet hostPath: path: \"/var/lib/kubelet\" - name: var-lib-kube-scheduler hostPath: path: \"/var/lib/kube-scheduler\" - name: var-lib-kube-controller-manager hostPath: path: \"/var/lib/kube-controller-manager\" - name: etc-systemd hostPath: path: \"/etc/systemd\" - name: lib-systemd hostPath: path: \"/lib/systemd\" - name: srv-kubernetes hostPath: path: \"/srv/kubernetes\" - name: etc-kubernetes hostPath: path: \"/etc/kubernetes\" - name: usr-bin hostPath: path: \"/usr/bin\" - name: etc-cni-netd hostPath: path: \"/etc/cni/net.d/\" - name: opt-cni-bin hostPath: path: \"/opt/cni/bin/\" EOF 5.2.1 Wait for a few seconds for the job to complete $ kubectl get pods 5.2.2 The results are held in the pod's logs kubectl logs kube-bench . . . 5.3 Kube Hunter 5.3.1 Installing kube-hunter to check from inside the cluster $ kubectl create -f https://raw.githubusercontent.com/aquasecurity/kube-hunter/master/job.yaml $ kubectl get pods $ kubectl logs <kube-hunter-pod-name> 6.0 Fetch Juice-Shop Helm Chart and scan with Checkov and Helm lint 6.1 Install Helm mkdir helm && cd helm curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash helm repo update 6.2 Add the helm repo for Juice-shop $ helm repo add juice https://charts.securecodebox.io 6.3 Inspect Juiceshop Helm Chart $ helm fetch juice/juice-shop $ tar -xvf juice-shop-3.15.2.tgz $ cd $ helm lint juice-shop/ $ pip install checkov $ checkov -d juice-shop/ --framework helm 7.0 Install JuiceShop Application with Helm chart $ helm install juice-shop-helm juice/juice-shop Additional Challenges \u00b6 Additional Vulnerability Scanning - Install Scanbox and run the security scans we did in previous section with it Automate Testing - Write a bash script to automate the security testing Take the helm - Write and audit a helm chart fot the StudentBook application","title":"1.K8s, Kind and Security Scanning"},{"location":"walk/kubernetes/#1k8s-kind-and-security-scanning","text":"","title":"1.K8s, Kind and Security Scanning"},{"location":"walk/kubernetes/#introduction","text":"Kubernetes (often shortened to \"K8s\") is an open-source container orchestration system. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation. Kubernetes allows developers to easily deploy, scale, and manage containerized applications in a clustered environment. Kubernetes provides a set of abstractions that are used to define, deploy, and manage containerized applications. These abstractions include pods, the smallest deployable units in Kubernetes, and are used to group one or more containers. Services, which provide stable network connections to pods and Replication Controllers, ensure that the desired number of pod replicas are running at all times. Kubernetes also provides load balancing, automatic scaling, and self-healing to ensure that applications are highly available and can handle changing traffic patterns. Additionally, Kubernetes can be integrated with other tools and services, such as monitoring and logging, to provide a complete solution for managing containerized applications. To develop with Kubernetes clusters locally, we will use KIND Kubernetes IN Docker (kind). Overall, KIND is a powerful tool for local Kubernetes development because it is easy to set up and use, provides an isolated environment for testing and debugging, and integrates well with other tools. Other options, such as minikube, work equally as well.","title":"Introduction"},{"location":"walk/kubernetes/#supplementary-learning-materials","text":"Additional Links: Kubernetes and Kind - https://www.baeldung.com/ops/kubernetes-kind Kubernetes and Kind - https://phoenixnap.com/kb/kubernetes-kind Kubernetes cluster development with kind - https://faun.pub/creating-a-kubernetes-cluster-for-development-with-kind-189df2cb0792 Kind with podman - https://www.linkedin.com/pulse/kind-podman-ubuntu-2004lts-ludger-pottmeier Kind & K8s lab - https://cloudyuga.guru/hands_on_lab/kind-k8s Juiceshop on Kubernetes - https://networkandcode.hashnode.dev/run-owasp-juice-shop-as-a-kubernetes-service Ingress for Kubernetes - https://kind.sigs.k8s.io/docs/user/ingress/#ingress-nginx Kubescape - https://www.howtogeek.com/devops/how-to-scan-for-kubernetes-vulnerabilities-with-kubescape/ Security scanning for K8s - https://mattermost.com/blog/the-top-7-open-source-tools-for-securing-your-kubernetes-cluster/#kubeaudit Helm - https://helm.sh/docs/intro/quickstart/","title":"Supplementary Learning Materials"},{"location":"walk/kubernetes/#scenario","text":"Create a named Cluster called JuiceShop Create or copy deployment manifest and service manifest and sabe for Juiceshop (provided above) Scan Juice Manifests with Kubescape Install Juice shop with manifests Complete a security scan of the Kubernetes environment (Include: Kubesecurity bench, Kubeaudit, Kubescape) Fetch Juice-Shop Helm Chart and scan with Checkov and Helm lint Install JuiceShop Application with Helm chart Install StudentBook Using manifests Juice-Shop Manifests You are free to write these on your own or you can just copy and paste ``` cat <<'EOF'>> juice-shop-manifests.yaml kind: Deployment apiVersion: apps/v1 metadata: name: juice-shop-manifests spec: template: metadata: labels: app: juice-shop-manifests spec: containers: - name: juice-shop-manifests image: bkimminich/juice-shop selector: matchLabels: app: juice-shop-manifests --- kind: Service apiVersion: v1 metadata: name: juice-shop-manifests spec: type: NodePort selector: app: juice-shop-manifests ports: - name: http port: 8000 targetPort: 3000 --- EOF ``` Solution 1.0 Create a Cluster named Juice with NGNIX Support 1.1 Install Kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind # If you are going to use podman then you need to add it as an external provider in your ~/.bashrc or ~/.zshrc file $ export KIND_EXPERIMENTAL_PROVIDER=podman # If you like, you can alos add an alias to the docker command to your ~/.bashrc or ~/.zshrc file alias docker='podman' # Other things you may need to do can be found here https://kind.sigs.k8s.io/docs/user/rootless/ 1.2 Create Cluster default Kind cat <<EOF | kind create cluster --name studentbook --config=- kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.25.3@sha256:f52781bc0d7a19fb6c405c2af83abfeb311f130707a0e219175677e366cc45d1 kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \"ingress-ready=true\" extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP EOF 2.0 Create JuiceSHop Manifests (do not deploy yet) cat <<'EOF'>> juice-shop-manifests.yaml kind: Deployment apiVersion: apps/v1 metadata: name: juice-shop-manifests spec: template: metadata: labels: app: juice-shop-manifests spec: containers: - name: juice-shop-manifests image: bkimminich/juice-shop selector: matchLabels: app: juice-shop-manifests --- kind: Service apiVersion: v1 metadata: name: juice-shop-manifests spec: type: NodePort selector: app: juice-shop-manifests ports: - name: http port: 8000 targetPort: 3000 --- EOF 3.0 Scan Juice Manifests with Kubescape curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash kubescape scan framework nsa k8s/*.yaml 4.0 Deploy Juiceshop via manifests 4.1 Create deployment kubectl create -f juice-shop-manifests.yaml 4.2 Watch service kubectl get po --watch 4.3 If want to browse by local pod IP then do following # Check service endpoints kubectl get svc juice-shop # get internal IP kubectl get no -o wide | awk '{print $6}' 4.4 If you want to browse via local host then do the following kubectl port-forward svc/juice-shop-manifests 8080:8000 5.0 Complete a security scan of the Kubernetes environment 5.1 Using Kubescape curl -s https://raw.githubusercontent.com/kubescape/kubescape/master/install.sh | /bin/bash # scan everything $ kubescape scan --submit --enable-host-scan --format-version v2 --verbose # scan Manifests $ kubescape scan framework nsa *.yaml # scan specific file $ kubescape scan framework nsa --exclude-namespaces kube-system,kube-public 5.2 Kubesecurity Bench kubectl apply -f - <<EOF --- apiVersion: batch/v1 kind: Job metadata: name: kube-bench spec: template: metadata: labels: app: kube-bench spec: hostPID: true containers: - name: kube-bench image: docker.io/aquasec/kube-bench:v0.6.10 command: [\"kube-bench\"] volumeMounts: - name: var-lib-etcd mountPath: /var/lib/etcd readOnly: true - name: var-lib-kubelet mountPath: /var/lib/kubelet readOnly: true - name: var-lib-kube-scheduler mountPath: /var/lib/kube-scheduler readOnly: true - name: var-lib-kube-controller-manager mountPath: /var/lib/kube-controller-manager readOnly: true - name: etc-systemd mountPath: /etc/systemd readOnly: true - name: lib-systemd mountPath: /lib/systemd/ readOnly: true - name: srv-kubernetes mountPath: /srv/kubernetes/ readOnly: true - name: etc-kubernetes mountPath: /etc/kubernetes readOnly: true # /usr/local/mount-from-host/bin is mounted to access kubectl / kubelet, for auto-detecting the Kubernetes version. # You can omit this mount if you specify --version as part of the command. - name: usr-bin mountPath: /usr/local/mount-from-host/bin readOnly: true - name: etc-cni-netd mountPath: /etc/cni/net.d/ readOnly: true - name: opt-cni-bin mountPath: /opt/cni/bin/ readOnly: true restartPolicy: Never volumes: - name: var-lib-etcd hostPath: path: \"/var/lib/etcd\" - name: var-lib-kubelet hostPath: path: \"/var/lib/kubelet\" - name: var-lib-kube-scheduler hostPath: path: \"/var/lib/kube-scheduler\" - name: var-lib-kube-controller-manager hostPath: path: \"/var/lib/kube-controller-manager\" - name: etc-systemd hostPath: path: \"/etc/systemd\" - name: lib-systemd hostPath: path: \"/lib/systemd\" - name: srv-kubernetes hostPath: path: \"/srv/kubernetes\" - name: etc-kubernetes hostPath: path: \"/etc/kubernetes\" - name: usr-bin hostPath: path: \"/usr/bin\" - name: etc-cni-netd hostPath: path: \"/etc/cni/net.d/\" - name: opt-cni-bin hostPath: path: \"/opt/cni/bin/\" EOF 5.2.1 Wait for a few seconds for the job to complete $ kubectl get pods 5.2.2 The results are held in the pod's logs kubectl logs kube-bench . . . 5.3 Kube Hunter 5.3.1 Installing kube-hunter to check from inside the cluster $ kubectl create -f https://raw.githubusercontent.com/aquasecurity/kube-hunter/master/job.yaml $ kubectl get pods $ kubectl logs <kube-hunter-pod-name> 6.0 Fetch Juice-Shop Helm Chart and scan with Checkov and Helm lint 6.1 Install Helm mkdir helm && cd helm curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash helm repo update 6.2 Add the helm repo for Juice-shop $ helm repo add juice https://charts.securecodebox.io 6.3 Inspect Juiceshop Helm Chart $ helm fetch juice/juice-shop $ tar -xvf juice-shop-3.15.2.tgz $ cd $ helm lint juice-shop/ $ pip install checkov $ checkov -d juice-shop/ --framework helm 7.0 Install JuiceShop Application with Helm chart $ helm install juice-shop-helm juice/juice-shop","title":"Scenario"},{"location":"walk/kubernetes/#additional-challenges","text":"Additional Vulnerability Scanning - Install Scanbox and run the security scans we did in previous section with it Automate Testing - Write a bash script to automate the security testing Take the helm - Write and audit a helm chart fot the StudentBook application","title":"Additional Challenges"},{"location":"walk/verification/","text":"4.Image Verification in K8s(WIP) \u00b6 Introduction \u00b6 We will use Kyverno to validate that only signed images are deployed to Kubernetes. Kyverno is an open-source policy engine for Kubernetes that allows you to define and enforce policies across your entire cluster. It allows you to define policies using Kubernetes native resources, such as ConfigMaps and Custom Resources, and it can be used to validate, mutate, and generate resources based on those policies. Kyverno provides a robust set of features for policy management, including: Policy validation: Kyverno allows you to define validation policies that can be used to check that resources conform to your desired state. Policy mutation: Kyverno allows you to define mutation policies that can be used to automatically modify resources as they are created or updated. Policy generation: Kyverno allows you to define generation policies that can automatically generate new resources based on existing resources. Here's an example of how you could use Kyverno to enforce the use of signed images: Create a policy that defines the validation rules for image signatures. This policy could check for the presence of a signature and verify that the signature is valid and was created by a trusted source. Apply the policy to the relevant resources in the cluster. For example, you could apply the policy to all pods or deployments, or you could apply it to specific namespaces or labels. Implement the policy by creating a Kyverno admission webhook. This webhook will intercept all requests to create or update resources in the cluster and apply the policy to ensure that only signed images are allowed to run. Configure your image registry to provide the signature information for images that are pushed to it. Once this policy is in place, any attempt to deploy an unsigned image will be rejected by the Kyverno admission webhook, ensuring that only signed images can run in the cluster. Supplementary Learning Material \u00b6 Work in progress ... Scenario \u00b6 Sign Image with Cosign and verify signature Sign Image and Verify signature Generate, sign, and attach SBOM Install Kyverno Verify all images in registry Create a deployment using an unsigned image Create deployment using signed image Try (and fail) to deploy unsigned image to K8s Work in progress ... . Solution \u00b6 Work in progress .. Additional Challenges \u00b6","title":"4.Image Verification in K8s(WIP)"},{"location":"walk/verification/#4image-verification-in-k8swip","text":"","title":"4.Image Verification in K8s(WIP)"},{"location":"walk/verification/#introduction","text":"We will use Kyverno to validate that only signed images are deployed to Kubernetes. Kyverno is an open-source policy engine for Kubernetes that allows you to define and enforce policies across your entire cluster. It allows you to define policies using Kubernetes native resources, such as ConfigMaps and Custom Resources, and it can be used to validate, mutate, and generate resources based on those policies. Kyverno provides a robust set of features for policy management, including: Policy validation: Kyverno allows you to define validation policies that can be used to check that resources conform to your desired state. Policy mutation: Kyverno allows you to define mutation policies that can be used to automatically modify resources as they are created or updated. Policy generation: Kyverno allows you to define generation policies that can automatically generate new resources based on existing resources. Here's an example of how you could use Kyverno to enforce the use of signed images: Create a policy that defines the validation rules for image signatures. This policy could check for the presence of a signature and verify that the signature is valid and was created by a trusted source. Apply the policy to the relevant resources in the cluster. For example, you could apply the policy to all pods or deployments, or you could apply it to specific namespaces or labels. Implement the policy by creating a Kyverno admission webhook. This webhook will intercept all requests to create or update resources in the cluster and apply the policy to ensure that only signed images are allowed to run. Configure your image registry to provide the signature information for images that are pushed to it. Once this policy is in place, any attempt to deploy an unsigned image will be rejected by the Kyverno admission webhook, ensuring that only signed images can run in the cluster.","title":"Introduction"},{"location":"walk/verification/#supplementary-learning-material","text":"Work in progress ...","title":"Supplementary Learning Material"},{"location":"walk/verification/#scenario","text":"Sign Image with Cosign and verify signature Sign Image and Verify signature Generate, sign, and attach SBOM Install Kyverno Verify all images in registry Create a deployment using an unsigned image Create deployment using signed image Try (and fail) to deploy unsigned image to K8s Work in progress ... .","title":"Scenario"},{"location":"walk/verification/#solution","text":"Work in progress ..","title":"Solution"},{"location":"walk/verification/#additional-challenges","text":"","title":"Additional Challenges"}]}